{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarinaOhm/master_thesis/blob/main/RICE_FORECASTING_SCRIPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Master thesis\n",
        "## *Forecasting of Sales data on SKU level*\n",
        "*Developed by Max Hedeman Gueniau, Niklas Madsen, and Marina Ohm*"
      ],
      "metadata": {
        "id": "DppLWHYftdS9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install and import libraries"
      ],
      "metadata": {
        "id": "JV4hqSnmtiwz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install libraries"
      ],
      "metadata": {
        "id": "al5FQ6uXtqQW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sd8vukU4qEbi",
        "outputId": "60315499-85e4-4a81-ef00-ec7d4d4f81e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fbprophet\n",
            "  Using cached fbprophet-0.7.1.tar.gz (64 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: Cython>=0.22 in /usr/local/lib/python3.10/dist-packages (from fbprophet) (0.29.34)\n",
            "Collecting cmdstanpy==0.9.5 (from fbprophet)\n",
            "  Using cached cmdstanpy-0.9.5-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: pystan>=2.14 in /usr/local/lib/python3.10/dist-packages (from fbprophet) (3.7.0)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.10/dist-packages (from fbprophet) (1.22.4)\n",
            "Requirement already satisfied: pandas>=1.0.4 in /usr/local/lib/python3.10/dist-packages (from fbprophet) (1.5.3)\n",
            "Requirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from fbprophet) (3.7.1)\n",
            "Requirement already satisfied: LunarCalendar>=0.0.9 in /usr/local/lib/python3.10/dist-packages (from fbprophet) (0.0.9)\n",
            "Requirement already satisfied: convertdate>=2.1.2 in /usr/local/lib/python3.10/dist-packages (from fbprophet) (2.4.0)\n",
            "Requirement already satisfied: holidays>=0.10.2 in /usr/local/lib/python3.10/dist-packages (from fbprophet) (0.23)\n",
            "Collecting setuptools-git>=1.2 (from fbprophet)\n",
            "  Using cached setuptools_git-1.2-py2.py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.0 in /usr/local/lib/python3.10/dist-packages (from fbprophet) (2.8.2)\n",
            "Requirement already satisfied: tqdm>=4.36.1 in /usr/local/lib/python3.10/dist-packages (from fbprophet) (4.65.0)\n",
            "Requirement already satisfied: pymeeus<=1,>=0.3.13 in /usr/local/lib/python3.10/dist-packages (from convertdate>=2.1.2->fbprophet) (0.5.12)\n",
            "Requirement already satisfied: hijri-converter in /usr/local/lib/python3.10/dist-packages (from holidays>=0.10.2->fbprophet) (2.3.1)\n",
            "Requirement already satisfied: korean-lunar-calendar in /usr/local/lib/python3.10/dist-packages (from holidays>=0.10.2->fbprophet) (0.3.1)\n",
            "Requirement already satisfied: ephem>=3.7.5.3 in /usr/local/lib/python3.10/dist-packages (from LunarCalendar>=0.0.9->fbprophet) (4.1.4)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from LunarCalendar>=0.0.9->fbprophet) (2022.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->fbprophet) (1.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->fbprophet) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->fbprophet) (4.39.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->fbprophet) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->fbprophet) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->fbprophet) (8.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->fbprophet) (3.0.9)\n",
            "Requirement already satisfied: aiohttp<4.0,>=3.6 in /usr/local/lib/python3.10/dist-packages (from pystan>=2.14->fbprophet) (3.8.4)\n",
            "Requirement already satisfied: clikit<0.7,>=0.6 in /usr/local/lib/python3.10/dist-packages (from pystan>=2.14->fbprophet) (0.6.2)\n",
            "Requirement already satisfied: httpstan<4.11,>=4.10 in /usr/local/lib/python3.10/dist-packages (from pystan>=2.14->fbprophet) (4.10.0)\n",
            "Requirement already satisfied: pysimdjson<6.0.0,>=5.0.2 in /usr/local/lib/python3.10/dist-packages (from pystan>=2.14->fbprophet) (5.0.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from pystan>=2.14->fbprophet) (67.7.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.0->fbprophet) (1.16.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.6->pystan>=2.14->fbprophet) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.6->pystan>=2.14->fbprophet) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.6->pystan>=2.14->fbprophet) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.6->pystan>=2.14->fbprophet) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.6->pystan>=2.14->fbprophet) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.6->pystan>=2.14->fbprophet) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0,>=3.6->pystan>=2.14->fbprophet) (1.3.1)\n",
            "Requirement already satisfied: crashtest<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from clikit<0.7,>=0.6->pystan>=2.14->fbprophet) (0.3.1)\n",
            "Requirement already satisfied: pastel<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from clikit<0.7,>=0.6->pystan>=2.14->fbprophet) (0.2.1)\n",
            "Requirement already satisfied: pylev<2.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from clikit<0.7,>=0.6->pystan>=2.14->fbprophet) (1.4.0)\n",
            "Requirement already satisfied: appdirs<2.0,>=1.4 in /usr/local/lib/python3.10/dist-packages (from httpstan<4.11,>=4.10->pystan>=2.14->fbprophet) (1.4.4)\n",
            "Requirement already satisfied: marshmallow<4.0,>=3.10 in /usr/local/lib/python3.10/dist-packages (from httpstan<4.11,>=4.10->pystan>=2.14->fbprophet) (3.19.0)\n",
            "Requirement already satisfied: webargs<9.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from httpstan<4.11,>=4.10->pystan>=2.14->fbprophet) (8.2.0)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.0->aiohttp<4.0,>=3.6->pystan>=2.14->fbprophet) (3.4)\n",
            "Building wheels for collected packages: fbprophet\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for fbprophet (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for fbprophet\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[?25h  Running setup.py clean for fbprophet\n",
            "Failed to build fbprophet\n",
            "\u001b[31mERROR: Could not build wheels for fbprophet, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install pystan numpy pandas matplotlib Cython setuptools\n",
        "!pip install pmdarima statsmodels h5py keras-tuner\n",
        "!pip install pystan~=2.14\n",
        "!pip install --no-cache-dir fbprophet"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import libraries"
      ],
      "metadata": {
        "id": "LrW7QhY9tyTw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kVrucAbt6UPL",
        "outputId": "6d575d86-5bfc-4dbd-ef76-144d1c9bef1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd, numpy as np, seaborn as sns, time, matplotlib.pyplot as plt, plotly.express as px, random, plotly.graph_objs as go, xgboost as xgb, lightgbm as lgb, tensorflow as tf\n",
        "import math, warnings, pmdarima, statsmodels, pickle, time, logging, os, itertools, keras, keras_tuner as kt\n",
        "from fbprophet import Prophet\n",
        "from statsmodels.tsa.arima.model import ARIMA, ARIMAResults\n",
        "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from xgboost import XGBRegressor\n",
        "import prophet.diagnostics\n",
        "from prophet.diagnostics import cross_validation, performance_metrics\n",
        "from pmdarima.arima import auto_arima\n",
        "from statsmodels.graphics.tsaplots import plot_acf\n",
        "from datetime import datetime\n",
        "from statsmodels.tsa.stattools import adfuller, acf\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_percentage_error\n",
        "from sklearn.model_selection import TimeSeriesSplit, RandomizedSearchCV, GridSearchCV\n",
        "from tensorflow.keras.models import Sequential, save_model, load_model\n",
        "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from keras_tuner import Objective\n",
        "from pickle import dump,load\n",
        "from sklearn.base import BaseEstimator, clone\n",
        "from scipy.stats import uniform\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from keras_tuner import Objective\n",
        "from scipy.stats import norm\n",
        "from matplotlib import rcParams\n",
        "from google.colab import drive\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "logging.getLogger('tensorflow').setLevel(logging.ERROR)\n",
        "logging.getLogger('fbprophet').setLevel(logging.ERROR)\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPfAlco4g0tb"
      },
      "source": [
        "# Sales forecasting"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Support functions applied in the train/test function and the forecast function"
      ],
      "metadata": {
        "id": "bvcwffvzwC83"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VV3Bjf4NjhjI"
      },
      "outputs": [],
      "source": [
        "def import_data(path):\n",
        "  \"\"\"\n",
        "  Import data through Gdrive path\n",
        "\n",
        "  :parameters:\n",
        "  - Path to data in Gdrive\n",
        "\n",
        "  :return:\n",
        "  - Imported data in DataFrame\n",
        "  \"\"\"\n",
        "\n",
        "  path_data = path\n",
        "  df = pd.read_csv(path_data)\n",
        "\n",
        "  # Subset only necessary columns \n",
        "  df = df[['_ItemNumber','date_','Q']]\n",
        "\n",
        "  return df\n",
        "\n",
        "\n",
        "def lstm_prepros_seq(data, seq=15):\n",
        "  \"\"\"\n",
        "  Function for preparing data for LSTM. Specifically by scaling it and converting it into sequences \n",
        "\n",
        "  :parameters:\n",
        "  - Training and testing data\n",
        "\n",
        "  :return:\n",
        "  - Array x with sequential input data\n",
        "  - Array y with sequential target data\n",
        "  - Scalar object for invert scaling\n",
        "  \"\"\"\n",
        "\n",
        "  # Scaling data \n",
        "  scaler = MinMaxScaler()\n",
        "  data = scaler.fit_transform(data)\n",
        "\n",
        "  # Defining objects to store input sequences and corresponding target values\n",
        "  x = []\n",
        "  y = []\n",
        "\n",
        "  # Looping through the data. The -seq-1 part is to ensure there is enough data points to create an input sequence of length seq AND a corresponding target value\n",
        "  for i in range(len(data)-seq-1):\n",
        "\n",
        "    # Breaking the data into smaller 'windows' over overlapping sequences\n",
        "    window = data[i:(i+seq),0] \n",
        "    x.append(window)\n",
        "    y.append(data[i+seq,0]) \n",
        "  \n",
        "  return np.array(x), np.array(y), scaler\n",
        "\n",
        "\n",
        "def model_tuning(hp, train_lstm_x, train_lstm_y):\n",
        "    \"\"\"\n",
        "    Function for hyperparameter tuning LSTM and finding the best performing NN architecture\n",
        "\n",
        "    :parameters:\n",
        "    - HP settings\n",
        "    - Training data x values\n",
        "    - Training data y values\n",
        "\n",
        "    :return:\n",
        "    - Best performing LSTM model\n",
        "    \"\"\"\n",
        "\n",
        "    # Initiate sequential class\n",
        "    model = Sequential()\n",
        "    \n",
        "    # Adding an LSTM layer to the model and testing different # of units ranging between a min and a max defined by a step. Input shape is defined by the shape of the train data\n",
        "    model.add(LSTM(hp.Int('input_unit', min_value=32, max_value=512, step=32), return_sequences=True, input_shape=(train_lstm_x.shape[1],train_lstm_x.shape[2])))\n",
        "\n",
        "    # Adding LSTM layers to the model and testing different # of units in each. \n",
        "    for i in range(hp.Int('n_layers', 1, 5)):\n",
        "        model.add(LSTM(hp.Int(f'lstm_{i}_units', min_value=32, max_value=512, step=32),return_sequences=True))\n",
        "    \n",
        "    # Adding final LSTM layer without return_sequences = True to ensure that a single value is returned for the next layer\n",
        "    model.add(LSTM(hp.Int('layer_2_neurons',min_value=32,max_value=512,step=32)))\n",
        "\n",
        "    # A dropout layer and testing different dropout values\n",
        "    model.add(Dropout(hp.Float('Dropout_rate',min_value=0,max_value=0.5,step=0.1)))\n",
        "\n",
        "    # Adding a dense layer with 30 neurons added and with the activation function being either relu or sigmoid\n",
        "    model.add(Dense(30, activation=hp.Choice('dense_activation',values=['relu', 'sigmoid'],default='relu')))\n",
        "\n",
        "    # Adding output layer with the activation function being either relu or sigmoid. The output layer returns one neuron, as the model is expected to output a single value.\n",
        "    model.add(Dense(1, activation=hp.Choice('dense_activation',values=['relu', 'sigmoid'],default='relu')))\n",
        "   \n",
        "    # Compiling model using ADAM and MSE\n",
        "    model.compile(loss='mean_squared_error', optimizer='adam' ,metrics = ['mse'])\n",
        "    \n",
        "    return model\n",
        "    \n",
        "    \n",
        "\n",
        "def preprocess(df, sku):\n",
        "    \"\"\"\n",
        "    Preprocessing function to make the SKU level data ready for further modelling\n",
        "\n",
        "    :parameters:\n",
        "    - Full DataFrame\n",
        "    - String with the itemnumber of the SKU to be processed\n",
        "\n",
        "    :returns:\n",
        "    - A preprocessed, and stationary SKU DataFrame\n",
        "    - An object containing the level of differencing conducted\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    np.random.seed(42)\n",
        "\n",
        "    # Subsetting SKU data from full df \n",
        "    sku_df = df[df['_ItemNumber']==sku].drop('_ItemNumber', axis=1)\n",
        "\n",
        "    # Convert date_ column to datetime object and use as index\n",
        "    sku_df['date_'] = pd.to_datetime(sku_df['date_'])\n",
        "    sku_df.set_index('date_', inplace=True)\n",
        "\n",
        "    # Resample df to weekly frequency and sum the values of the Q column\n",
        "    sku_df = sku_df.resample('W', label='right', closed='right').sum()\n",
        "\n",
        "    # Impute missing values using linear interpolation\n",
        "    sku_df = sku_df.interpolate(method='linear')\n",
        "\n",
        "    # Plot original TS\n",
        "    fig, axs = plt.subplots(1, 3, figsize=(20,5))\n",
        "    axs[0].plot(sku_df, label='Original')\n",
        "    axs[0].legend()\n",
        "    axs[0].set_title('Original Time Series')\n",
        "\n",
        "    # Copy the original df\n",
        "    sku_df_log = sku_df.copy()\n",
        "\n",
        "    # Apply log transformation to the 'Q' column in the new df, replace negative infinity values with NaN and impute \n",
        "    sku_df_log['Q'] = np.log1p(sku_df_log['Q'])\n",
        "    sku_df_log['Q'] = sku_df_log['Q'].replace(-np.inf, np.nan)\n",
        "    sku_df_log = sku_df_log.interpolate(method='linear')\n",
        "\n",
        "    # Plot log transformed TS\n",
        "    axs[1].plot(sku_df_log, label='Log applied')\n",
        "    axs[1].legend()\n",
        "    axs[1].set_title('Logarithmic data')\n",
        "\n",
        "\n",
        "    # Stationarity check using seperate function based on ADF\n",
        "    def check_stationarity(series):\n",
        "        result = adfuller(series)\n",
        "        significance_level = 0.05\n",
        "        return result[1] < significance_level\n",
        "\n",
        "    # Initial preprocess level = 0\n",
        "    preprocess_level = 0\n",
        "\n",
        "    # Run check_stationarity on log transformed ts\n",
        "    is_stationary = check_stationarity(sku_df_log['Q'])\n",
        "\n",
        "    # If data is not stationary, apply first differencing on both trend and seasonality and change preprocess level to 1\n",
        "    if not is_stationary:\n",
        "        sku_df_diff = sku_df_log.diff().diff(periods=1)\n",
        "        sku_df_diff.dropna(inplace=True)\n",
        "        preprocess_level = 1\n",
        "\n",
        "        # Run check_stationary on first difference data\n",
        "        is_stationary = check_stationarity(sku_df_diff['Q'])\n",
        "        \n",
        "        # If the data is stationary, continue, if not, apply second differencing on both trend and seasonality and change preprocess level to 2\n",
        "        if is_stationary:\n",
        "            print(\"The time series is now stationary after differencing.\")\n",
        "            sku_df_stationary = sku_df_diff\n",
        "\n",
        "        else:\n",
        "            sku_df_diff2 = sku_df_diff.diff().diff(periods=1)\n",
        "            sku_df_diff2.dropna(inplace=True)\n",
        "            preprocess_level = 2\n",
        "            \n",
        "            # Run check_stationary on second difference data\n",
        "            is_stationary = check_stationarity(sku_df_diff2['Q'])\n",
        "            \n",
        "            # If the data is stationary, continue, if not, print warning and apply second differencing on both trend and seasonality and change preprocess level to 2\n",
        "            if is_stationary:\n",
        "                print(\"The time series is now stationary after second differencing.\")\n",
        "                sku_df_stationary = sku_df_diff2\n",
        "            else:\n",
        "                print(\"The time series is still not stationary after second differencing. Keeping the second differencing data\")\n",
        "                sku_df_stationary = sku_df_diff2\n",
        "    \n",
        "    # If the data is stationary from the beginning, do nothing and keep preprocessing level at 0\n",
        "    else:\n",
        "        print(\"The time series is already stationary.\")\n",
        "        sku_df_stationary = sku_df_log\n",
        "\n",
        "    # Plot after stationarity check and conversion\n",
        "    axs[2].plot(sku_df_stationary, label='Stationary')\n",
        "    axs[2].legend()\n",
        "    axs[2].set_title('Stationary time series')\n",
        "    plt.show()\n",
        "\n",
        "    return sku_df_stationary, preprocess_level\n",
        "\n",
        "\n",
        "def metrics_calc_print(model, test_data, pred_data):\n",
        "    \"\"\"\n",
        "    Calculating performance metrics RMSE, MSE, and MAE\n",
        "\n",
        "    :parameters:\n",
        "    - Name of model given as string\n",
        "    - Test data\n",
        "    - Predicted data\n",
        "\n",
        "    :return:\n",
        "    - Print statement with metrics\n",
        "    - RMSE, MSE, MAE values\n",
        "    \"\"\"\n",
        "    print('Calculating metrics...')\n",
        "  \n",
        "    # Invert log transformation\n",
        "    test_data = np.expm1(test_data)\n",
        "    pred_data = np.expm1(pred_data)\n",
        "\n",
        "    # Impute negative values with 0 (or another chosen value)\n",
        "    test_data = np.where(test_data < 0, 0, test_data)\n",
        "    pred_data = np.where(pred_data < 0, 0, pred_data)\n",
        "    test_data = np.where(np.isinf(test_data), 0, test_data)\n",
        "    pred_data = np.where(np.isinf(pred_data), 0, pred_data)\n",
        "\n",
        "    # Calculate metrics\n",
        "    rmse = math.sqrt(mean_squared_error(test_data, pred_data))\n",
        "    mse =  mean_squared_error(test_data, pred_data)\n",
        "    mae = arima_mae = mean_absolute_error(test_data, pred_data)\n",
        "\n",
        "    print(f'{model} RMSE: {rmse}')\n",
        "    print(f'{model} MSE: {mse}')\n",
        "    print(f'{model} MAE: {mae}')\n",
        "\n",
        "    return rmse, mse, mae\n",
        "\n",
        "\n",
        "def run_arima(train, test):\n",
        "    \"\"\" \n",
        "    Function for training, tuning, and testing ARIMA\n",
        "\n",
        "    :parameters:\n",
        "    - Training data\n",
        "    - Test data\n",
        "\n",
        "    :returns:\n",
        "    - Best parameters\n",
        "    - Prediction data\n",
        "    - Test data\n",
        "    \"\"\"\n",
        "\n",
        "    print('\\n')\n",
        "    print('************************************')\n",
        "    print('ARIMA')\n",
        "    print('************************************')\n",
        "\n",
        "    np.random.seed(42)\n",
        "    \n",
        "    train_arima = train.copy()\n",
        "    test_arima = test.copy()\n",
        "\n",
        "    # Fitting arima model, return and extract the best parameters\n",
        "    arima_model = auto_arima(train_arima, return_best_params=True)\n",
        "     \n",
        "    # Extract the best ARIMA parameters\n",
        "    best_arima_model = (arima_model.order)\n",
        "\n",
        "    # Generate predictions for the test set\n",
        "    arima_test_predictions = arima_model.predict(n_periods=len(test_arima))\n",
        "\n",
        "    return best_arima_model, arima_test_predictions, test_arima\n",
        "\n",
        "\n",
        "def run_es(train,test):\n",
        "    \"\"\" \n",
        "    Function for training, tuning, and testing Exponential smoothing\n",
        "\n",
        "    :parameters:\n",
        "    - Training data\n",
        "    - Test data\n",
        "\n",
        "    :returns:\n",
        "    - Best parameters\n",
        "    - Prediction data\n",
        "    - Test data\n",
        "    \"\"\"\n",
        "    print('\\n')\n",
        "    print('************************************')\n",
        "    print('Exponential smoothing')\n",
        "    print('************************************')\n",
        "\n",
        "    np.random.seed(42)\n",
        "\n",
        "    # Define the hyperparameters to tune over\n",
        "    alpha_range_es = np.linspace(0.1, 1, 5)\n",
        "    beta_range_es = np.linspace(0.1, 1, 5)\n",
        "    gamma_range_es = np.linspace(0.1, 1, 5)\n",
        "    trend_options = ['add', 'mul', None]\n",
        "    seasonal_options = ['add', 'mul', None]\n",
        "    damped_trend_options = [True, False]\n",
        "\n",
        "    # Create hyperparametergrid\n",
        "    hyperparameters_es = [(alpha, beta, gamma, trend, seasonal, damped_trend) for alpha in alpha_range_es for beta in beta_range_es for gamma in gamma_range_es for trend in trend_options for seasonal in seasonal_options for damped_trend in damped_trend_options]\n",
        "\n",
        "    train_es = train.copy()\n",
        "    test_es = test.copy()\n",
        "\n",
        "    best_rmse = float('inf')\n",
        "\n",
        "    # Train and test models with different hyperparameter combinations to find the combination with the best RMSE\n",
        "    for alpha, beta, gamma, trend, seasonal, damped_trend in hyperparameters_es:\n",
        "\n",
        "        try:\n",
        "            model = ExponentialSmoothing(train_es, trend=trend, seasonal=seasonal, seasonal_periods=12, damped_trend=damped_trend).fit(smoothing_level=alpha, smoothing_slope=beta, smoothing_seasonal=gamma)\n",
        "            es_validation_predictions = model.forecast(len(test_es))\n",
        "            es_rmse = math.sqrt(mean_squared_error(test_es, es_validation_predictions))\n",
        "\n",
        "            # If the current combination has an RMSE that is lower than the current best_rmse then these hyperparameters should be saved\n",
        "            if es_rmse < best_rmse:\n",
        "                best_rmse = es_rmse\n",
        "                es_best_hyperparameters = (alpha, beta, gamma, trend, seasonal, damped_trend)\n",
        "\n",
        "        except ValueError:\n",
        "            continue\n",
        "\n",
        "    # Fit the model with the best hyperparameters on the training set\n",
        "    model = ExponentialSmoothing(train_es).fit(smoothing_level=es_best_hyperparameters[0], smoothing_slope=es_best_hyperparameters[1], smoothing_seasonal=es_best_hyperparameters[2])\n",
        "\n",
        "    # Generate predictions for the test set\n",
        "    es_test_predictions = model.forecast(len(test_es))\n",
        "\n",
        "    return es_best_hyperparameters, es_test_predictions, test_es\n",
        "\n",
        "def run_xgb(train, test, tscv):\n",
        "      \"\"\" \n",
        "      Function for training, tuning, and testing XGboost\n",
        "\n",
        "      :parameters:\n",
        "      - Training data\n",
        "      - Test data\n",
        "\n",
        "      :returns:\n",
        "      - Best parameters\n",
        "      - Prediction data\n",
        "      - Test data\n",
        "      \"\"\"\n",
        "      print('\\n')\n",
        "      print('************************************')\n",
        "      print('XGBOOST')\n",
        "      print('************************************')\n",
        "\n",
        "      # Define hyperparmeter grid\n",
        "      xgb_params = {\n",
        "          'learning_rate': [0.01, 0.05, 0.1, 0.2, 0.4],\n",
        "          'n_estimators': [10, 20, 50, 100, 200],\n",
        "          'max_depth': [3, 5, 7, 9, 12],\n",
        "          'colsample_bytree': [0.3, 0.4, 0.5, 0.7, 0.8],\n",
        "          'min_child_weight': [1, 3, 5, 7, 9],\n",
        "          'lambda': [0.01, 0.1, 1.0, 10.0, 100.0],\n",
        "          'alpha': [0.01, 0.1, 1.0, 10.0, 100.0]\n",
        "      }\n",
        "      \n",
        "      np.random.seed(42)\n",
        "\n",
        "      train_xbg=train.copy()\n",
        "      test_xbg=test.copy()\n",
        "      \n",
        "      # Converting index into unix time format as required for XGB\n",
        "      train_xbg.index=train_xbg.index.astype(np.int64) // 10**9\n",
        "      test_xbg.index=test_xbg.index.astype(np.int64) // 10**9\n",
        "\n",
        "      # Splitting into X and y for training and testing sets\n",
        "      xgb_train_x = train_xbg.index.values.reshape(-1, 1)\n",
        "      xgb_train_y = train_xbg['Q']\n",
        "      xgb_test_x = test_xbg.index.values.reshape(-1, 1)\n",
        "      xgb_test_y = test_xbg['Q']\n",
        "\n",
        "      # Initiate XGBoost object\n",
        "      xgboost_model = xgb.XGBRegressor(objective='reg:squarederror')\n",
        "\n",
        "      # Define the random search object. Input is the model, the parameters to search over, the cross-validation method to apply, and what scoring method that evaluates the performance of each set of hyperparameters\n",
        "      random_search = RandomizedSearchCV(xgboost_model, param_distributions=xgb_params, cv=tscv, scoring='neg_mean_squared_error', n_iter = 50)\n",
        "\n",
        "      #Convert the input features and target values to DMatrix objects\n",
        "      dmat_train = xgb.DMatrix(xgb_train_x, label=xgb_train_y)\n",
        "      dmat_test = xgb.DMatrix(xgb_test_x, label=xgb_test_y)\n",
        "      \n",
        "      # Find the best parameters based on the random_search object\n",
        "      random_search.fit(xgb_train_x,xgb_train_y)\n",
        "      best_xgboost_model = random_search.best_estimator_\n",
        "\n",
        "      #Generate predictions for the test set\n",
        "      xgboost_test_predictions = best_xgboost_model.predict(xgb_test_x)\n",
        "\n",
        "      return best_xgboost_model, xgboost_test_predictions, xgb_test_y\n",
        "\n",
        "\n",
        "def run_prophet(train, test):\n",
        "    \"\"\" \n",
        "    Function for training, tuning, and testing Prophet\n",
        "\n",
        "    :parameters:\n",
        "    - Training data\n",
        "    - Test data\n",
        "\n",
        "    :returns:\n",
        "    - Best parameters\n",
        "    - Prediction data\n",
        "    - Test data\n",
        "    \"\"\"\n",
        "    print('\\n')\n",
        "    print('************************************')\n",
        "    print('PROPHET')\n",
        "    print('************************************')\n",
        "\n",
        "    np.random.seed(42)\n",
        "\n",
        "    # Define hyperparmeter grid\n",
        "    proph_params = {\n",
        "        'seasonality_mode': ['additive', 'multiplicative'],\n",
        "        'changepoint_prior_scale': [0.001, 0.1],\n",
        "        'seasonality_prior_scale': [0.01, 1.0],\n",
        "        'changepoint_range': [0.8, 0.9],\n",
        "        'growth': ['linear', 'logistic']\n",
        "    }\n",
        "\n",
        "    train_proph = train.copy()\n",
        "    test_proph = test.copy()\n",
        "\n",
        "    # Renaming to required Prophet column titles\n",
        "    train_proph = train_proph.reset_index().rename(columns={\"date_\": \"ds\",\"Q\": \"y\"})\n",
        "    test_proph = test_proph.reset_index().rename(columns={\"date_\": \"ds\",\"Q\": \"y\"})\n",
        "\n",
        "    best_proph_params = None\n",
        "    best_prophet_rmse = None\n",
        "\n",
        "    # Loop over hyperparameter grid\n",
        "    for mode in proph_params['seasonality_mode']:\n",
        "        for prior_scale in proph_params['changepoint_prior_scale']:\n",
        "            for seasonal_scale in proph_params['seasonality_prior_scale']:\n",
        "                for changepoint in proph_params['changepoint_range']:\n",
        "                    for growth in proph_params['growth']:\n",
        "\n",
        "                        # Fit Prophet model with the selected hyperparameters\n",
        "                        prophet_model = Prophet(seasonality_mode=mode,\n",
        "                                                changepoint_prior_scale=prior_scale,\n",
        "                                                seasonality_prior_scale=seasonal_scale,\n",
        "                                                changepoint_range=changepoint,\n",
        "                                                growth=growth,\n",
        "                                                yearly_seasonality=True)\n",
        "                        prophet_model.fit(train_proph)\n",
        "\n",
        "                        # Generate predictions for the test set and calculate RMSE\n",
        "                        future = prophet_model.make_future_dataframe(periods=len(test_proph), freq='MS')\n",
        "                        future = future[-len(test_proph):] \n",
        "                        forecast = prophet_model.predict(future)\n",
        "                        prophet_rmse = math.sqrt(mean_squared_error(test_proph.y, forecast.yhat))\n",
        "\n",
        "                        # If the RMSE is lower than current best RMSE, overwrite the parameters\n",
        "                        if best_proph_params is None:\n",
        "                            best_proph_params = {'rmse': prophet_rmse, \n",
        "                                                    'seasonality_mode': mode, \n",
        "                                                    'changepoint_prior_scale': prior_scale, \n",
        "                                                    'seasonality_prior_scale': seasonal_scale, \n",
        "                                                    'changepoint_range': changepoint,\n",
        "                                                    'growth': growth}\n",
        "                        elif prophet_rmse < best_proph_params['rmse']:\n",
        "                            best_proph_params = {'rmse': prophet_rmse, \n",
        "                                                    'seasonality_mode': mode, \n",
        "                                                    'changepoint_prior_scale': prior_scale, \n",
        "                                                    'seasonality_prior_scale': seasonal_scale, \n",
        "                                                    'changepoint_range': changepoint,\n",
        "                                                    'growth': growth}\n",
        "                        elif prophet_rmse > best_proph_params['rmse']:\n",
        "                            pass\n",
        "\n",
        "    return best_proph_params, forecast, test_proph\n",
        "\n",
        "\n",
        "def run_lstm(train, test, sku):\n",
        "      \"\"\" \n",
        "      Function for training, tuning, and testing Prophet\n",
        "\n",
        "      :parameters:\n",
        "      - Training data\n",
        "      - Test data\n",
        "\n",
        "      :returns:\n",
        "      - Best parameters\n",
        "      - Prediction data\n",
        "      - Test data\n",
        "      \"\"\"\n",
        "      print('\\n')\n",
        "      print('************************************')\n",
        "      print('LSTM')\n",
        "      print('************************************')\n",
        "\n",
        "      np.random.seed(42)\n",
        "\n",
        "      train_lstm = train.copy()\n",
        "      test_lstm = test.copy()\n",
        "\n",
        "      # Try except block to handle SKUs with fewer historical datapoints\n",
        "      try:\n",
        "          # Preprocessing input data into seq 15 using support function\n",
        "          train_lstm_x,train_lstm_y,scaler_train = lstm_prepros_seq(train_lstm, 15)\n",
        "          test_lstm_x,test_lstm_y,scaler_test = lstm_prepros_seq(test_lstm, 15)\n",
        "\n",
        "          # LSTM requires a 3D shape input (samples, features, timesteps) \n",
        "          # samples = number of observations, features = 1 (univariate analysis), timesteps = number of timesteps in each sequence so equal to seq parameter\n",
        "          train_lstm_x = np.reshape(train_lstm_x, (train_lstm_x.shape[0], train_lstm_x.shape[1], 1))\n",
        "          test_lstm_x = np.reshape(test_lstm_x, (test_lstm_x.shape[0], test_lstm_x.shape[1], 1))\n",
        "          \n",
        "          # Hyperparametertuning using model_tuning function\n",
        "          objective = Objective('mse', direction='min')\n",
        "\n",
        "          # using timestamp as unique_id for given model\n",
        "          unique_id = int(time.time())\n",
        "          tuner = kt.RandomSearch(lambda hp: model_tuning(hp, train_lstm_x, train_lstm_y), objective=objective, max_trials = 3, executions_per_trial = 1, directory=f\"./{unique_id}\")\n",
        "\n",
        "          # Define the checkpoint callback to save the best model\n",
        "          checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(f\"{sku}_{unique_id}_lstm_best_model.h5\", monitor='val_loss', save_best_only=True, save_weights_only=False, mode='auto', save_freq='epoch')\n",
        "\n",
        "          # Set up the checkpoint directory\n",
        "          checkpoint_dir = os.path.dirname(f\"{sku}_{unique_id}_lstm_best_model.h5\")\n",
        "\n",
        "          # Define the EarlyStopping callback\n",
        "          early_stopping_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, mode='min', restore_best_weights=True)\n",
        "\n",
        "          # Train the model with the checkpoint callback\n",
        "          tuner.search(x=train_lstm_x, y=train_lstm_y, epochs=15, batch_size=64, validation_data=(test_lstm_x, test_lstm_y), callbacks=[checkpoint_callback, early_stopping_callback])\n",
        "\n",
        "          # Extracting the best model\n",
        "          saved_best_model_path = f\"{sku}_{unique_id}_lstm_best_model.h5\"\n",
        "          lstm_best_model = lstm_best_model = load_model(saved_best_model_path)\n",
        "\n",
        "          # Using the best model to predict using the test set\n",
        "          test_pred = lstm_best_model.predict(test_lstm_x)\n",
        "\n",
        "          # Undo scaling of variables before calculating metrics\n",
        "          test_y = scaler_test.inverse_transform(test_lstm_y.reshape(-1, 1))\n",
        "          test_pred = scaler_test.inverse_transform(test_pred)\n",
        "\n",
        "      # If not enough data for seq = 15, trying with a smaller window (seq = 5)\n",
        "      except IndexError:\n",
        "        try:\n",
        "            train_lstm_x, train_lstm_y, scaler_train = lstm_prepros_seq(train_lstm, 5)\n",
        "            test_lstm_x, test_lstm_y, scaler_test = lstm_prepros_seq(test_lstm, 5)\n",
        "\n",
        "            train_lstm_x = np.reshape(train_lstm_x, (train_lstm_x.shape[0], train_lstm_x.shape[1], 1))\n",
        "            test_lstm_x = np.reshape(test_lstm_x, (test_lstm_x.shape[0], test_lstm_x.shape[1], 1))\n",
        "\n",
        "            # Hyperparametertuning using model_tuning function\n",
        "            objective = Objective('mse', direction='min')\n",
        "\n",
        "            # using timestamp as unique_id for given model\n",
        "            unique_id = int(time.time())\n",
        "            tuner = kt.RandomSearch(lambda hp: model_tuning(hp, train_lstm_x, train_lstm_y), objective=objective, max_trials = 3, executions_per_trial = 1, directory=f\"./{unique_id}\")\n",
        "\n",
        "            # Define the checkpoint callback to save the best model\n",
        "            checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(f\"{sku}_{unique_id}_lstm_best_model.h5\", monitor='val_loss', save_best_only=True, save_weights_only=False, mode='auto', save_freq='epoch')\n",
        "\n",
        "            # Set up the checkpoint directory\n",
        "            checkpoint_dir = os.path.dirname(f\"{sku}_{unique_id}_lstm_best_model.h5\")\n",
        "\n",
        "            # Train the model with the checkpoint callback\n",
        "            tuner.search(x=train_lstm_x, y=train_lstm_y, epochs=15, batch_size=64, validation_data=(test_lstm_x, test_lstm_y), callbacks=[checkpoint_callback])\n",
        "\n",
        "            # Extracting the best model\n",
        "            saved_best_model_path = f\"{sku}_{unique_id}_lstm_best_model.h5\"\n",
        "            lstm_best_model = lstm_best_model = load_model(saved_best_model_path)\n",
        "\n",
        "            # Using the best model to predict using the test set\n",
        "            test_pred = lstm_best_model.predict(test_lstm_x)\n",
        "\n",
        "            # Undo scaling of variables before calculating metrics\n",
        "            test_y = scaler_test.inverse_transform(test_lstm_y.reshape(-1, 1))\n",
        "            test_pred = scaler_test.inverse_transform(test_pred)\n",
        "\n",
        "        # if seq = 5 not feasible either, the model will be skipped and metrics set at -1\n",
        "        except IndexError:\n",
        "            lstm_rmse, lstm_mse, lstm_mae = -1, -1, -1\n",
        "            lstm_best_model = None\n",
        "            test_pred = None\n",
        "            test_y = None\n",
        "\n",
        "      return lstm_best_model, test_pred, test_y\n",
        "\n",
        "def invert_preprocessing(y_pred, preprocess_level, periods_to_forecast, sku_df):\n",
        "    \"\"\"\n",
        "    Function to invert preprocessing, specifically the differencing conducted to make the series stationary\n",
        "    Applied after forecasting\n",
        "\n",
        "    :parameters:\n",
        "    - Predicted values\n",
        "    - Preprocessing level \n",
        "    - Number of periods forecasted\n",
        "    - SKU level dataframe\n",
        "\n",
        "    :return:\n",
        "    - Inverted predictions\n",
        "    \"\"\"\n",
        "\n",
        "    y_pred = np.expm1(y_pred)\n",
        "\n",
        "    # If first difference was conducted\n",
        "    if preprocess_level == 1:\n",
        "      y_pred = np.cumsum(y_pred)[-periods_to_forecast:]\n",
        "      y_pred = np.insert(y_pred, 0, sku_df['Q'].iloc[-1])\n",
        "      y_pred = y_pred[1:]\n",
        "\n",
        "    # If second difference was conducted\n",
        "    elif preprocess_level == 2:\n",
        "      y_pred = np.cumsum(y_pred)[-periods_to_forecast:]\n",
        "      y_pred = np.insert(y_pred, 0, sku_df['Q'].iloc[-1])\n",
        "      y_pred = np.cumsum(y_pred)\n",
        "      y_pred = np.insert(y_pred, 0, sku_df['Q'].iloc[-2])\n",
        "      y_pred = y_pred[-periods_to_forecast:]\n",
        "      y_pred = y_pred\n",
        "\n",
        "    # If no difference was conducted\n",
        "    else:\n",
        "      y_pred = y_pred[-periods_to_forecast:]\n",
        "      \n",
        "    return y_pred\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0dkSb6u2xmt"
      },
      "source": [
        "## Main function for training and testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HV4SGovzg2F7"
      },
      "outputs": [],
      "source": [
        "def train_test_models(df):\n",
        "  \"\"\"\n",
        "  Main function for training, tuning and testing ARIMA, ETS, XGB, Prophet and LSTM on SKU level\n",
        "  The main function applies the following support functions: preprocess(), lstm_prepros_seq(), model_tuning(), run_arima(), run_es(), run_xgb(), run_prophet(), run_lstm()\n",
        "\n",
        "  :parameters:\n",
        "  - DataFrame containing historical sales for all SKUs\n",
        "\n",
        "  :return:\n",
        "  - Dictionary containing the performance of all 5 models for all SKUs\n",
        "  \"\"\"\n",
        "\n",
        "  np.random.seed(42)\n",
        "\n",
        "  # Include only relevant columns\n",
        "  df = df[['_ItemNumber','Q','date_']]\n",
        "\n",
        "  # Initate dictionary to store performance metrics\n",
        "  output_dict = {}\n",
        "\n",
        "  # Looping through each SKU\n",
        "  for sku in df['_ItemNumber'].unique():\n",
        "\n",
        "    # Initiating outer dict to store specific SKU information\n",
        "    output_dict[sku]={'metrics': {}, 'parameters': {}}\n",
        "\n",
        "    print('************************************')\n",
        "    print(f'Processing SKU:{sku}')\n",
        "    print('************************************')\n",
        "    print('\\n')\n",
        "\n",
        "    # Preprocess SKU df and save the level of preprocessing for later inverting\n",
        "    sku_df, preprocess_level = preprocess(df, sku)\n",
        "\n",
        "    # Cross validation using TimeSeriesSplit to make sure we keep the chronological order. For each SKU it will create 3 splits of the data into training and testing sets\n",
        "    n_splits = 3\n",
        "    tscv = TimeSeriesSplit(n_splits=n_splits)  \n",
        "    \n",
        "    # List comprehension for storing RMSE, MSE and MAE for each fold\n",
        "    arima_all_rmse, es_all_rmse, xgboost_all_rmse, prophet_all_rmse, lstm_all_rmse, arima_all_mse, es_all_mse, xgboost_all_mse, prophet_all_mse, lstm_all_mse, arima_all_mae, es_all_mae, xgboost_all_mae, prophet_all_mae, lstm_all_mae = [[] for _ in range(15)]  \n",
        "\n",
        "\n",
        "    ######### Training and testing #########\n",
        "    # For each model: train, tune, test, calculate metrics, and append to metric lists\n",
        "    for fold, (train_index, test_index) in enumerate(tscv.split(sku_df)):\n",
        "      \n",
        "      num_folds = tscv.get_n_splits()\n",
        "      \n",
        "      print(f'FOLD {fold + 1} / {num_folds} ')\n",
        "\n",
        "      # Split the data into train and test sets for the current fold\n",
        "      train = sku_df.iloc[train_index]\n",
        "      test = sku_df.iloc[test_index]\n",
        "      \n",
        "      ######### ARIMA #########\n",
        "      \n",
        "      best_arima_model, arima_test_predictions, test_arima = run_arima(train, test)\n",
        "  \n",
        "      arima_rmse, arima_mse, arima_mae = metrics_calc_print('ARIMA', test_arima, arima_test_predictions)\n",
        "\n",
        "      arima_all_rmse.append(arima_rmse)\n",
        "      arima_all_mse.append(arima_mse)\n",
        "      arima_all_mae.append(arima_mae)\n",
        "\n",
        "\n",
        "      ######### EXP SMOOTHING #########\n",
        "\n",
        "      es_best_hyperparameters, es_test_predictions, test_es = run_es(train, test)\n",
        "\n",
        "      es_rmse, es_mse, es_mae = metrics_calc_print('EXPONENTIAL SMOOTHING', test_es, es_test_predictions)\n",
        "\n",
        "      es_all_rmse.append(es_rmse)\n",
        "      es_all_mse.append(es_mse)\n",
        "      es_all_mae.append(es_mae)\n",
        "\n",
        "\n",
        "      ######### XGBOOST #########\n",
        "\n",
        "      try:\n",
        "        best_xgboost_model, xgboost_test_predictions, xgb_test_y = run_xgb(train, test, tscv)\n",
        "\n",
        "        xgboost_rmse, xgboost_mse, xgboost_mae = metrics_calc_print('XGBOOST', xgb_test_y, xgboost_test_predictions)\n",
        "\n",
        "        xgboost_all_rmse.append(xgboost_rmse)\n",
        "        xgboost_all_mse.append(xgboost_mse)\n",
        "        xgboost_all_mae.append(xgboost_mae)\n",
        "\n",
        "      except ValueError as ve:\n",
        "          print(f\"Skipping XGBoost for SKU {sku}: {ve}\")\n",
        "          continue\n",
        "\n",
        "\n",
        "      ######### Prophet #########\n",
        "\n",
        "      prophet_best_model, forecast, test_proph = run_prophet(train, test)\n",
        "\n",
        "      prophet_rmse, prophet_mse, prophet_mae = metrics_calc_print('PROPHET', test_proph.y, forecast.yhat)\n",
        "\n",
        "      prophet_all_rmse.append(prophet_rmse)\n",
        "      prophet_all_mse.append(prophet_mse)\n",
        "      prophet_all_mae.append(prophet_mae)\n",
        "      \n",
        "      ###### LSTM ######\n",
        "\n",
        "      lstm_best_model, test_predictions, test_y = run_lstm(train, test, sku)\n",
        "\n",
        "      if test_y is None:\n",
        "          lstm_rmse, lstm_mse, lstm_mae = -1, -1, -1\n",
        "          print('Warning: no metrics calculated. Too little data to process sequentially')\n",
        "\n",
        "      else:\n",
        "          lstm_rmse, lstm_mse, lstm_mae = metrics_calc_print('LSTM', test_y, test_predictions)\n",
        "\n",
        "      lstm_all_rmse.append(lstm_rmse)\n",
        "      lstm_all_mse.append(lstm_mse)\n",
        "      lstm_all_mae.append(lstm_mae)\n",
        "\n",
        "    print('\\n')\n",
        "    print('Finalizing results...')\n",
        "    print('\\n')\n",
        "    print(f'Average scores across {num_folds} folds')\n",
        "    print('************************************')\n",
        "    \n",
        "    ######### Saving results #########\n",
        "\n",
        "    # Define the list of models and their corresponding metric lists\n",
        "    models = ['arima', 'es', 'xgboost', 'prophet', 'lstm']\n",
        "    all_metrics = {'rmse': [arima_all_rmse, es_all_rmse, xgboost_all_rmse, prophet_all_rmse, lstm_all_rmse],\n",
        "                    'mse': [arima_all_mse, es_all_mse, xgboost_all_mse, prophet_all_mse, lstm_all_mse],\n",
        "                    'mae': [arima_all_mae, es_all_mae, xgboost_all_mae, prophet_all_mae, lstm_all_mae]}\n",
        "\n",
        "    # Calculate the average performance of the models across all folds\n",
        "    averaged_metrics = {model: {metric: np.nanmean(all_metrics[metric][i]) for metric in all_metrics} for i, model in enumerate(models)}\n",
        "\n",
        "    # Saving metrics for each model\n",
        "    for model in models:\n",
        "      output_dict[sku]['metrics'][model] = averaged_metrics[model]\n",
        "\n",
        "    # Saving best parameters for each model in the output dict\n",
        "    output_dict[sku]['parameters']['arima'] = best_arima_model\n",
        "    output_dict[sku]['parameters']['es'] = es_best_hyperparameters\n",
        "    output_dict[sku]['parameters']['xgboost'] = best_xgboost_model.get_params()\n",
        "    output_dict[sku]['parameters']['prophet'] = prophet_best_model\n",
        "    if lstm_best_model:\n",
        "        output_dict[sku]['parameters']['lstm'] = lstm_best_model.get_config()\n",
        "    else:\n",
        "        output_dict[sku]['parameters']['lstm'] = None\n",
        "\n",
        "\n",
        "    print('\\n')\n",
        "    print('Updated dictionary...')\n",
        "    \n",
        "  return output_dict\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWvBiOl420lS"
      },
      "source": [
        "## Main function for forecasting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0YEJxEt76nJW"
      },
      "outputs": [],
      "source": [
        "def deploy_forecast(model_dict, df, periods_to_forecast):\n",
        "    \"\"\"\n",
        "    Main function for deploying forecast based on the output from the training and testing function\n",
        "    The best performing model based on RMSE is applied for the final forecast\n",
        "    \n",
        "    :parameters:\n",
        "    - Output dictionary from train_test function with performance metrics on SKU level\n",
        "    - DataFrame with SKU level sales data\n",
        "    - Number of periods to forecast\n",
        "\n",
        "    :returns:\n",
        "    - \n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    np.random.seed(42) \n",
        "\n",
        "    forecast_df = pd.DataFrame(columns=['_ItemNumber', 'Model', 'Forecast', 'ConfInt_lower', 'ConfInt_upper', 'train_test_RMSE'])\n",
        "\n",
        "    unique_skus = df['_ItemNumber'].unique()\n",
        "\n",
        "    ######### Deploy forecast #########\n",
        "    for sku in unique_skus:\n",
        "\n",
        "        # Subsetting SKU level data\n",
        "        sku_df = df[df['_ItemNumber'] == sku]\n",
        "\n",
        "        # Preprocess data\n",
        "        sku_df, preprocess_level = preprocess(sku_df, sku)\n",
        "\n",
        "        # Extract information about best_model and best_model metrics\n",
        "        metrics = model_dict[sku]['metrics']\n",
        "\n",
        "        # Find then best model as the one with the minimum RMSE and extract its respective best parameters\n",
        "        best_model = min([model for model in metrics if metrics[model]['rmse'] >= 0], key=lambda x: metrics[x]['rmse'])\n",
        "        best_params = model_dict[sku]['parameters'][best_model]\n",
        "        best_performance = model_dict[sku]['metrics'][best_model]['rmse']\n",
        "        \n",
        "        # Deploy forecast based on best model\n",
        "        if best_model == 'arima':\n",
        "            print(f'Forecasting sku {sku} using ARIMA')\n",
        "\n",
        "            # Extracting best parameters\n",
        "            p, d, q = best_params\n",
        "            model = ARIMA(sku_df, order=(p, d, q))\n",
        "            results = model.fit()\n",
        "            \n",
        "            # Forecast values + confidence intervals\n",
        "            forecast_obj = results.get_forecast(steps=periods_to_forecast, alpha=0.05)\n",
        "            forecast = forecast_obj.predicted_mean \n",
        "            conf_int = forecast_obj.conf_int()\n",
        "\n",
        "            # Invert preprocessing \n",
        "            forecast = forecast.values\n",
        "            forecast = invert_preprocessing(forecast, preprocess_level, periods_to_forecast, sku_df)\n",
        "            conf_int_lower = invert_preprocessing(conf_int.iloc[:,0].values, preprocess_level, periods_to_forecast, sku_df)\n",
        "            conf_int_upper = invert_preprocessing(conf_int.iloc[:,1].values, preprocess_level, periods_to_forecast, sku_df)\n",
        "            conf_int = pd.DataFrame({'ConfInt_lower':conf_int_lower , 'ConfInt_upper':conf_int_upper, 'train_test_RMSE':best_performance})\n",
        "\n",
        "            # Creating a DataFrame to store the forecast results incl confidenct intervalt\n",
        "            forecast_index = pd.date_range(start=sku_df.index[-1] + pd.Timedelta(days=7), periods=periods_to_forecast + 1, closed='right', freq='W')\n",
        "            sku_forecast = pd.DataFrame({'_ItemNumber': sku, 'Model': 'ARIMA', 'Forecast': forecast})\n",
        "            sku_forecast = sku_forecast.join(conf_int)\n",
        "            sku_forecast = sku_forecast.set_index(forecast_index)\n",
        "\n",
        "            # Renaming the conf_int columns to align with main dataframe\n",
        "            sku_forecast = sku_forecast.rename(columns={'lower Q': 'ConfInt_lower', 'upper Q': 'ConfInt_upper'})\n",
        "            \n",
        "            # Appending forecast results to the forecast_df\n",
        "            forecast_df = pd.concat([forecast_df, sku_forecast], axis=0)\n",
        "\n",
        "            print(sku)\n",
        "            print(sku_forecast)\n",
        "            \n",
        "\n",
        "        elif best_model == 'es':\n",
        "            print(f'Forecasting sku {sku} using Exponential Smoothing')\n",
        "            \n",
        "            sku_df = sku_df.clip(lower=0.01)\n",
        "            \n",
        "            # Extract best parameters\n",
        "            alpha, beta, gamma, trend, seasonal, damped_trend = best_params\n",
        "\n",
        "            # First trying to fit the strand and the seasonal component\n",
        "            try:\n",
        "                model = ExponentialSmoothing(sku_df, trend=trend, seasonal=seasonal, damped_trend=damped_trend)\n",
        "                results = model.fit(smoothing_level=alpha, smoothing_slope=beta, smoothing_seasonal=gamma)\n",
        "\n",
        "            # If there is not sufficient data to fit the seasonal component, thus the specific error gets thrown, the seasonal component gets removes\n",
        "            except ValueError as e:\n",
        "                if 'Cannot compute initial seasonals using heuristic method' in str(e):\n",
        "                    \n",
        "                    model = ExponentialSmoothing(sku_df, trend=trend, seasonal=None, damped_trend=damped_trend)\n",
        "\n",
        "                    results = model.fit(smoothing_level=alpha, smoothing_slope=beta)\n",
        "                \n",
        "                # If the valueerror does not equal the above, then the alternative error should get thrown\n",
        "                else:\n",
        "                    raise e\n",
        "             \n",
        "            # Create forecasts      \n",
        "            forecast = results.forecast(periods_to_forecast)\n",
        "\n",
        "            # Invert preprocessing\n",
        "            forecast = forecast.values\n",
        "            forecast = invert_preprocessing(forecast, preprocess_level, periods_to_forecast, sku_df)\n",
        "\n",
        "            # Calculate the confidence intervals manually\n",
        "            stderr = np.std(results.resid, ddof=1)  \n",
        "            conf_int = np.outer(forecast, np.array([1, 1])) + stderr * np.array([-1.96, 1.96])\n",
        "\n",
        "            # Create dataframe to store forecasts\n",
        "            forecast_index = pd.date_range(start=sku_df.index[-1] + pd.Timedelta(days=7), periods=periods_to_forecast + 1, closed='right', freq='W')\n",
        "            sku_forecast = pd.DataFrame({'_ItemNumber': sku, 'Model': 'EXP SMOOTH', 'Forecast': forecast, 'ConfInt_lower': conf_int[:, 0], 'ConfInt_upper': conf_int[:, 1], 'train_test_RMSE':best_performance})\n",
        "            sku_forecast = sku_forecast.set_index(forecast_index)\n",
        "\n",
        "            # Appending forecast results to the forecast_df\n",
        "            forecast_df = pd.concat([forecast_df, sku_forecast], axis=0)\n",
        "\n",
        "            print(sku)\n",
        "            print(sku_forecast)\n",
        "            \n",
        "        \n",
        "        elif best_model == 'xgboost':\n",
        "            print(f'Forecasting sku {sku} using XGBOOST')\n",
        "\n",
        "            # Reset the index and convert the index to a new column 'Date'\n",
        "            sku_df.reset_index(inplace=True)\n",
        "            sku_df.rename(columns={'index': 'date_'}, inplace=True)\n",
        "            \n",
        "            # Convert date_ to unix timestamp because xgb only takes numerival data\n",
        "            sku_df['date_'] = sku_df['date_'].astype(np.int64) // 10**9\n",
        "\n",
        "            # Create the X and y matrices for the model\n",
        "            X = sku_df['date_']\n",
        "            y = sku_df['Q']\n",
        "\n",
        "            # Create XGBoost model with best parameters\n",
        "            booster = xgb.train(params=best_params, dtrain=xgb.DMatrix(X, label=y))\n",
        "\n",
        "            # Creating the forecast horizon date range for the forecasts\n",
        "            start_date = pd.to_datetime(sku_df['date_'], unit='s').max() + pd.Timedelta(weeks=1)\n",
        "            end_date = start_date + pd.Timedelta(weeks=periods_to_forecast)\n",
        "            forecast_dates = pd.date_range(start=start_date, end=end_date, freq='W')\n",
        "\n",
        "            # Create the forecast_dates_df DataFrame with the dates as the 'date_' column\n",
        "            forecast_dates_df = pd.DataFrame({'date_': forecast_dates})\n",
        "            forecast_dates_df['date_'] = forecast_dates_df['date_'].astype(int)\n",
        "\n",
        "            # Convert the forecast_dates_df DataFrame to a DMatrix object\n",
        "            forecast_dmatrix = xgb.DMatrix(data=forecast_dates_df.values)\n",
        "\n",
        "            # Make predictions using the pre-trained XGBoost model\n",
        "            forecast = booster.predict(forecast_dmatrix, ntree_limit=booster.best_ntree_limit)\n",
        "            forecast_stdevs = booster.predict(forecast_dmatrix, ntree_limit=booster.best_ntree_limit, pred_contribs=True).std(axis=1)[:periods_to_forecast]\n",
        "\n",
        "            # Invert preprocessing\n",
        "            forecast = invert_preprocessing(forecast, preprocess_level, periods_to_forecast, sku_df)\n",
        "\n",
        "            # Calculate upper and lower bounds of 95% confidence interval\n",
        "            upper_bound = forecast + 1.96 * forecast_stdevs\n",
        "            lower_bound = forecast - 1.96 * forecast_stdevs\n",
        "\n",
        "            # Combine forecast and confidence intervals into DataFrame\n",
        "            sku_df.index = pd.to_datetime(sku_df['date_'], unit='s')\n",
        "            forecast_index = pd.date_range(start=sku_df.index[-1] + pd.Timedelta(days=7), periods=periods_to_forecast + 1, closed='right', freq='W')\n",
        "            sku_forecast = pd.DataFrame({'_ItemNumber': sku, 'Model': 'XGBOOST', 'Forecast': forecast, 'ConfInt_lower': upper_bound, 'ConfInt_upper': lower_bound,'train_test_RMSE':best_performance })\n",
        "            sku_forecast = sku_forecast.set_index(forecast_index)\n",
        "\n",
        "            # Appending forecast results to the forecast_df\n",
        "            forecast_df = pd.concat([forecast_df, sku_forecast], axis=0)\n",
        "\n",
        "            print(sku)\n",
        "            print(sku_forecast)\n",
        "            \n",
        "        \n",
        "        elif best_model == 'prophet':\n",
        "            print(f'Forecasting sku {sku} using prophet')\n",
        "\n",
        "            # Change naming convention to fit to proplet\n",
        "            sku_df_prophet = pd.DataFrame({'ds': sku_df.index, 'y': sku_df.values.flatten()})\n",
        "\n",
        "            # Train model with best parameters\n",
        "            model = Prophet(changepoint_prior_scale=best_params['changepoint_prior_scale'], seasonality_prior_scale=best_params['seasonality_prior_scale'], changepoint_range=best_params['changepoint_range'], seasonality_mode=best_params['seasonality_mode'], interval_width=0.95)\n",
        "            model.fit(sku_df_prophet)\n",
        "\n",
        "            # Make weekly predictions\n",
        "            future = model.make_future_dataframe(periods=periods_to_forecast, freq='W',include_history=False)\n",
        "            forecast = model.predict(future)\n",
        "\n",
        "            # Invert preprocessing\n",
        "            forecast['yhat'] = invert_preprocessing(forecast['yhat'], preprocess_level, periods_to_forecast, sku_df)\n",
        "\n",
        "            # Calculate the confidence intervals\n",
        "            conf_int = forecast[['yhat_lower', 'yhat_upper']].values\n",
        "            conf_int = invert_preprocessing(conf_int, preprocess_level, periods_to_forecast, sku_df)\n",
        "            forecast['yhat_lower'] = conf_int[:, 0]\n",
        "            forecast['yhat_upper'] = conf_int[:, 1]\n",
        "            \n",
        "            # Create forecast df\n",
        "            sku_forecast = pd.DataFrame({'date': forecast['ds'], '_ItemNumber': sku, 'Model': 'PROPHET', 'Forecast': forecast['yhat'], 'ConfInt_lower': forecast['yhat_lower'], 'ConfInt_upper': forecast['yhat_upper'],'train_test_RMSE':best_performance })\n",
        "            sku_forecast = sku_forecast.set_index('date')\n",
        "\n",
        "            # Appending forecast results to the forecast_df\n",
        "            forecast_df = pd.concat([forecast_df, sku_forecast], axis=0)\n",
        "            print(sku)\n",
        "            print(sku_forecast)\n",
        "\n",
        "\n",
        "        elif best_model == 'lstm':\n",
        "            print(f'Forecasting sku {sku} using LSTM')\n",
        "\n",
        "\n",
        "            np.random.seed(42)\n",
        "            tf.random.set_seed(42)\n",
        "            \n",
        "            # Create range of dates used for the forecasting df later\n",
        "            forecast_index = pd.date_range(start=sku_df.index[-1], periods=periods_to_forecast + 1, closed='right', freq='W')\n",
        "\n",
        "            # Retrieving best parameters and configuring the model\n",
        "            model_params = model_dict[sku]['parameters'][best_model]\n",
        "            model = keras.models.Sequential.from_config(model_params)\n",
        "            model.compile(loss='mse', optimizer='adam')\n",
        "\n",
        "            # Determine the sequence length based on the trained LSTM model's input shape\n",
        "            seq_length = model.layers[0].input_shape[1]\n",
        "\n",
        "            # Preprocessing input data into sequential\n",
        "            X, y, scaler = lstm_prepros_seq(sku_df, seq=seq_length)\n",
        "\n",
        "            # Reshape X\n",
        "            X = np.reshape(X, (X.shape[0], X.shape[1], 1))\n",
        "\n",
        "            # Fit model\n",
        "            model.fit(X, y, epochs=20, verbose=0)\n",
        "\n",
        "            # Make out-of-sample forecasts\n",
        "            X_forecast = sku_df['Q'][-seq_length:].values.reshape(1, seq_length, 1)\n",
        "            y_pred = []\n",
        "\n",
        "            for i in range(periods_to_forecast):\n",
        "                forecast = model.predict(X_forecast)\n",
        "                y_pred.append(forecast[0, 0])\n",
        "                X_forecast = np.roll(X_forecast, -1)\n",
        "                X_forecast[0, -1, 0] = forecast[0, 0]\n",
        "\n",
        "\n",
        "            y_pred = np.array(y_pred)\n",
        "\n",
        "            # Invert scaling\n",
        "            y_pred = scaler.inverse_transform(y_pred.reshape(-1, 1)).flatten()\n",
        "\n",
        "            # Invert preprocessing\n",
        "            y_pred = invert_preprocessing(y_pred ,preprocess_level, periods_to_forecast, sku_df)\n",
        "            \n",
        "            # Calculate residuals \n",
        "            residuals = y[-periods_to_forecast:] - y_pred.flatten()\n",
        "            se = np.std(residuals)\n",
        "\n",
        "            # Calculate the confidence intervals\n",
        "            alpha = 0.05\n",
        "            z = norm.ppf(1 - alpha / 2)\n",
        "            lower_bounds = y_pred - z * se\n",
        "            upper_bounds = y_pred + z * se\n",
        "\n",
        "            sku_forecast = pd.DataFrame({'_ItemNumber':sku, 'Model': 'LSTM', 'Forecast': y_pred.flatten(),'ConfInt_lower': lower_bounds.flatten(),'ConfInt_upper': upper_bounds.flatten(),'train_test_RMSE':best_performance }, index=forecast_index)\n",
        "\n",
        "            forecast_df = pd.concat([forecast_df, sku_forecast], axis=0)\n",
        "            \n",
        "            print(sku)\n",
        "            print(sku_forecast)\n",
        "        \n",
        "    return forecast_df"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}