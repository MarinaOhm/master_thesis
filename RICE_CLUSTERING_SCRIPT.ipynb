{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarinaOhm/master_thesis/blob/main/RICE_CLUSTERING_SCRIPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CBxKbynwjYb"
      },
      "source": [
        "# Master thesis\n",
        "## *Clustering for product portfolio management*\n",
        "*Developed by Max Hedeman Gueniau, Niklas Madsen, and Marina Ohm*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDb44kh3wgbR"
      },
      "source": [
        "# Install and import libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing libraries"
      ],
      "metadata": {
        "id": "-GCeMn2umIjy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJCA6B6WMHkR",
        "outputId": "f53f43a6-06f4-47bb-97ca-d6d8b50d7702"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fuzzy-c-means\n",
            "  Downloading fuzzy_c_means-1.7.0-py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: joblib<2.0.0,>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from fuzzy-c-means) (1.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from fuzzy-c-means) (1.22.4)\n",
            "Requirement already satisfied: pydantic<2.0.0,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from fuzzy-c-means) (1.10.7)\n",
            "Requirement already satisfied: tabulate<0.9.0,>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from fuzzy-c-means) (0.8.10)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from fuzzy-c-means) (4.65.0)\n",
            "Collecting typer<0.5.0,>=0.4.0 (from fuzzy-c-means)\n",
            "  Downloading typer-0.4.2-py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2.0.0,>=1.9.0->fuzzy-c-means) (4.5.0)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.5.0,>=0.4.0->fuzzy-c-means) (8.1.3)\n",
            "Installing collected packages: typer, fuzzy-c-means\n",
            "  Attempting uninstall: typer\n",
            "    Found existing installation: typer 0.7.0\n",
            "    Uninstalling typer-0.7.0:\n",
            "      Successfully uninstalled typer-0.7.0\n",
            "Successfully installed fuzzy-c-means-1.7.0 typer-0.4.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.1.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting hdbscan\n",
            "  Downloading hdbscan-0.8.29.tar.gz (5.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cython>=0.27 in /usr/local/lib/python3.10/dist-packages (from hdbscan) (0.29.34)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from hdbscan) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.10/dist-packages (from hdbscan) (1.10.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20 in /usr/local/lib/python3.10/dist-packages (from hdbscan) (1.2.2)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.10/dist-packages (from hdbscan) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20->hdbscan) (3.1.0)\n",
            "Building wheels for collected packages: hdbscan\n",
            "  Building wheel for hdbscan (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hdbscan: filename=hdbscan-0.8.29-cp310-cp310-linux_x86_64.whl size=3541430 sha256=7167ca6e8b8833e7e213f7361886411295fd05972779d9e33d3784058c3302d6\n",
            "  Stored in directory: /root/.cache/pip/wheels/dc/52/e3/6c6b60b126b4d5c4370cb5ac071b82950f91649d62d72f7f56\n",
            "Successfully built hdbscan\n",
            "Installing collected packages: hdbscan\n",
            "Successfully installed hdbscan-0.8.29\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting umap\n",
            "  Downloading umap-0.1.1.tar.gz (3.2 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: umap\n",
            "  Building wheel for umap (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for umap: filename=umap-0.1.1-py3-none-any.whl size=3542 sha256=ad5803438f80d191f2953f65342131790a71ab8115d96d028f8069fbf47dfd50\n",
            "  Stored in directory: /root/.cache/pip/wheels/15/f1/28/53dcf7a309118ed35d810a5f9cb995217800f3f269ab5771cb\n",
            "Successfully built umap\n",
            "Installing collected packages: umap\n",
            "Successfully installed umap-0.1.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting umap-learn\n",
            "  Downloading umap-learn-0.5.3.tar.gz (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.2/88.2 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (1.22.4)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (1.10.1)\n",
            "Requirement already satisfied: numba>=0.49 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (0.56.4)\n",
            "Collecting pynndescent>=0.5 (from umap-learn)\n",
            "  Downloading pynndescent-0.5.10.tar.gz (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from umap-learn) (4.65.0)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.49->umap-learn) (0.39.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from numba>=0.49->umap-learn) (67.7.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from pynndescent>=0.5->umap-learn) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22->umap-learn) (3.1.0)\n",
            "Building wheels for collected packages: umap-learn, pynndescent\n",
            "  Building wheel for umap-learn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for umap-learn: filename=umap_learn-0.5.3-py3-none-any.whl size=82816 sha256=7e4df8f4609ef22f825e66bcfba9f1354934cd5e012f97b7380ce3cfa32e53ab\n",
            "  Stored in directory: /root/.cache/pip/wheels/a0/e8/c6/a37ea663620bd5200ea1ba0907ab3c217042c1d035ef606acc\n",
            "  Building wheel for pynndescent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pynndescent: filename=pynndescent-0.5.10-py3-none-any.whl size=55622 sha256=e5acdac0802ea578c878b4018620b94c7ef180259dc38da6564580d508c0d5f5\n",
            "  Stored in directory: /root/.cache/pip/wheels/4a/38/5d/f60a40a66a9512b7e5e83517ebc2d1b42d857be97d135f1096\n",
            "Successfully built umap-learn pynndescent\n",
            "Installing collected packages: pynndescent, umap-learn\n",
            "Successfully installed pynndescent-0.5.10 umap-learn-0.5.3\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting Counter\n",
            "  Downloading Counter-1.0.0.tar.gz (5.2 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: Counter\n",
            "  Building wheel for Counter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for Counter: filename=Counter-1.0.0-py3-none-any.whl size=5393 sha256=a9e38972c692f3f7a907ac2b392e84e651350816b52f3c0a9aaa1fa149ff1241\n",
            "  Stored in directory: /root/.cache/pip/wheels/e3/02/6d/d5c0838427a060718c6060ae4d24da95a0e0df0d7a3dab8040\n",
            "Successfully built Counter\n",
            "Installing collected packages: Counter\n",
            "Successfully installed Counter-1.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install fuzzy-c-means\n",
        "!pip install --upgrade scikit-learn\n",
        "!pip install hdbscan\n",
        "!pip install umap\n",
        "!pip install umap-learn\n",
        "!pip install Counter"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing libraries"
      ],
      "metadata": {
        "id": "mIKtrb7xmK2D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-JxvGjYq0F06",
        "outputId": "fd0e6dc8-566a-4987-c435-c5ac4c096785"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import hdbscan\n",
        "import plotly.express as px\n",
        "import umap\n",
        "import nltk\n",
        "import string\n",
        "import collections\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.graph_objs as go\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import AgglomerativeClustering, DBSCAN\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from wordcloud import WordCloud\n",
        "from plotly.subplots import make_subplots\n",
        "from google.colab import drive\n",
        "from fcmeans import FCM\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0v_9XcDh8REl"
      },
      "source": [
        "# Clustering with PCA\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gnMwDGqLG0Sz"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    \"\"\"\n",
        "    Main function for clustering analysis, including all steps necessary.\n",
        "    Change path to data and allow access to Gdrive for the function to execute.\n",
        "\n",
        "    :parameters: \n",
        "    - None\n",
        "    \n",
        "    :return: \n",
        "    - hca_means: Cluster means from HCA clustering\n",
        "    - hca_df: Original DataFrame with cluster labels from HCA attached\n",
        "    - hca_ss: HCA clustering silhouette score\n",
        "    - hdbscan_means: Cluster means from HDBSCAN clustering\n",
        "    - hdbscan_df: Original DataFrame with cluster labels from HDBSCAN attached\n",
        "    - hdbscan_ss: Cluster means from HDBSCAN clustering\n",
        "    \"\"\"\n",
        "\n",
        "    # Setting seed for reproducability \n",
        "    random.seed(42)\n",
        "    \n",
        "    # Load and preprocess data from Gdrive. Path to folder should be adjusted\n",
        "    drive.mount('/content/drive')\n",
        "    df = pd.read_csv('/content/drive/MyDrive/Master Thesis/final_updated_clustering_data.csv') \n",
        "\n",
        "    # Preprocess data using the dedicated preprocess function\n",
        "    df_preprocessed, scaler = preprocess(df)\n",
        "    \n",
        "    # Find PCA components that encapsulates >90% of the variance in the data and PCA transform accordingly\n",
        "    df_transformed, pca= perform_pca(df_preprocessed)\n",
        "\n",
        "    # Perform HCA clustering on PCA transformed data\n",
        "    hca_means, hca_df, hca_ss = perform_hca_clustering(df_transformed, pca, scaler, df_preprocessed)\n",
        "\n",
        "    # Perform DBSCAN clustering on PCA transformed data\n",
        "    hdbscan_means, hdbscan_df, hdbscan_ss = perform_hdbscan_clustering(df_transformed, pca, scaler, df_preprocessed)\n",
        "\n",
        "    # Chosen model only: Examine output dataframe through descriptive stastistics\n",
        "    descriptive_clust(hdbscan_df)\n",
        "\n",
        "    return hca_means, hca_df, hca_ss, hdbscan_means, hdbscan_df, hdbscan_ss\n",
        "\n",
        "\n",
        "def text_preprocess(df):\n",
        "    \"\"\"\n",
        "    Preprocessing function for textual data to extract additional information through clustering\n",
        "\n",
        "    :parameters:\n",
        "    - Dataframe with column '_description' containing textual data\n",
        "\n",
        "    :return: \n",
        "    - DataFrame with an additional column containing text cluster labels and wordclouds for visualization of the data\n",
        "    \"\"\"\n",
        "\n",
        "    # Define constants\n",
        "    min_cluster_sizes = range(10, 15) # hp range to tune\n",
        "    n_neighbors = range(10, 15) # hp range to tune\n",
        "    best_params = {} # for storing best hp\n",
        "    best_score = -1 # for storing the silhouette score for the best hp combination\n",
        "\n",
        "    # Initiate objects to be used in the function\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    stemmer = PorterStemmer()\n",
        "\n",
        "    # Tokenize text, remove punctuation and stopwords, and stem words\n",
        "    df['_description'] = df['_description'].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)).lower().split())\n",
        "    df['_description'] = df['_description'].apply(lambda x: [word for word in x if not word in stop_words])\n",
        "    df['_description'] = df['_description'].apply(lambda x: [stemmer.stem(word) for word in x])\n",
        "\n",
        "    # Perform TF-IDF weighting on the text columns to weight words according to their appearances\n",
        "    tfidf_vectorizer = TfidfVectorizer()\n",
        "    tfidf_data = tfidf_vectorizer.fit_transform(df['_description'].apply(lambda x: \" \".join(x)))\n",
        "\n",
        "    # Iterate over all hp combinations using a for loop\n",
        "    for min_cluster_size in min_cluster_sizes:\n",
        "        for n in n_neighbors:\n",
        "\n",
        "            # Dimensionality reduction using UMAP\n",
        "            umap_model = umap.UMAP(n_neighbors=n, min_dist=0.5, metric='euclidean', random_state=42)\n",
        "            umap_data = umap_model.fit_transform(tfidf_data)\n",
        "\n",
        "            # Clustering using HDBSCAN\n",
        "            clusterer = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size)\n",
        "            cluster_labels = clusterer.fit_predict(umap_data)\n",
        "\n",
        "            # Calculate silhouette score\n",
        "            score = silhouette_score(umap_data, cluster_labels)\n",
        "\n",
        "            # Compare scores and update parameters if score is higher than current\n",
        "            if score > best_score:\n",
        "                best_score = score\n",
        "                best_params['min_cluster_size'] = min_cluster_size\n",
        "                best_params['n_neighbors'] = n\n",
        "\n",
        "    # Use the best parameters to update the code\n",
        "    umap_model = umap.UMAP(n_neighbors=best_params['n_neighbors'], min_dist=0.5, metric='euclidean', random_state=42)\n",
        "    umap_data = umap_model.fit_transform(tfidf_data)\n",
        "    clusterer = hdbscan.HDBSCAN(min_cluster_size=best_params['min_cluster_size'])\n",
        "    cluster_labels = clusterer.fit_predict(umap_data)\n",
        "    df['cluster'] = cluster_labels\n",
        "\n",
        "    # Get number of observations in each cluster\n",
        "    cluster_counts = pd.Series(cluster_labels).value_counts()\n",
        "    print(cluster_counts)\n",
        "    \n",
        "    # Extract 10 most common words for each cluster + create wordcloud for each cluster with 50 most common words\n",
        "    for label in df['cluster'].unique():\n",
        "\n",
        "        print(f\"Cluster {label}:\")\n",
        "        cluster_df = df.loc[df['cluster'] == label]\n",
        "        words = [word for doc in cluster_df['_description'] for word in doc]\n",
        "\n",
        "        # Find 10 most common words\n",
        "        word_counts = collections.Counter(words)\n",
        "        top_10_words = word_counts.most_common(10)\n",
        "        print('Top 10 words in clusters:')\n",
        "        print(top_10_words)\n",
        "\n",
        "        # Create word cloud\n",
        "        print(f'Wordcloud for cluster {label}')\n",
        "        word_counts = collections.Counter(words)\n",
        "        top_50_words = word_counts.most_common(50)\n",
        "        top_words_dict = dict(top_50_words)\n",
        "        wordcloud = WordCloud(width = 800, height = 800, background_color ='white', stopwords = stop_words, min_font_size = 10).generate_from_frequencies(top_words_dict)\n",
        "\n",
        "        # Plot word cloud\n",
        "        plt.figure(figsize = (8, 8), facecolor = None)\n",
        "        plt.imshow(wordcloud)\n",
        "        plt.axis(\"off\")\n",
        "        plt.tight_layout(pad = 0)\n",
        "        plt.show()\n",
        "    \n",
        "    return df\n",
        "\n",
        "\n",
        "def preprocess(df):\n",
        "    \"\"\"\n",
        "    Dedicated preprocessing function for handling numerical, categorical, and textual data (:using dedicated text_preprocessing function)\n",
        "\n",
        "    :parameters:\n",
        "    - DataFrame used for clustering\n",
        "\n",
        "    :return:\n",
        "    - Preprocessed DataFrame\n",
        "    - Scaler object from MinMaxScaler for later inverting\n",
        "    \"\"\"\n",
        "\n",
        "    # Initiate objects\n",
        "    scaler = MinMaxScaler()\n",
        "\n",
        "    # Removing all -inf profit_margins since this these are percieved data errors\n",
        "    df = df[(df['profit_margin'] >= 0) & (df['profit_margin'] != float('inf')) & (df['profit_margin'] != float('-inf'))]\n",
        "\n",
        "    # Dropping unnecessary columns\n",
        "    exclude_columns = [\"Unnamed: 0\",\"BDLRI Sales Status\"]\n",
        "    df = df.loc[:, ~df.columns.isin(exclude_columns)]\n",
        "\n",
        "    # One-hot encode categorical columns\n",
        "    df = pd.concat([df, pd.get_dummies(df['Country_Region of Origin Code'], prefix='Country')], axis=1).drop('Country_Region of Origin Code', axis=1)\n",
        "\n",
        "    # Preprocess textual data using pre-defined function\n",
        "    df = text_preprocess(df)\n",
        "\n",
        "    # One-got encode the newly added cluster_text column from the text_preprocess function\n",
        "    df = pd.concat([df, pd.get_dummies(df['cluster'], prefix='TC')], axis=1).drop('cluster', axis=1)\n",
        "\n",
        "    # Scale numerical values\n",
        "    df[['average_monthly_sales','average_order_size','average_value_by_order','unique_customer_per_item','average_monthly_quantity','profit_margin','Net Weight','Reorder Quantity','Unit Cost','First Purch Order Quantum','average_days_between_sales','Customer_life_time','average_monthly_sales_growth','sales_variability']] = scaler.fit_transform(df[['average_monthly_sales','average_order_size','average_value_by_order','unique_customer_per_item','average_monthly_quantity','profit_margin','Net Weight','Reorder Quantity','Unit Cost','First Purch Order Quantum','average_days_between_sales','Customer_life_time','average_monthly_sales_growth','sales_variability']])\n",
        "\n",
        "    # Remove and handle nan rows\n",
        "    df = df.dropna(subset=['_ItemNumber'])\n",
        "    df = df.fillna(0)\n",
        "    df = df.set_index('_ItemNumber')\n",
        "    df.columns = df.columns.astype(str)\n",
        "\n",
        "    # Dropping _description as information have been extracted from it through the text_preprocessing function\n",
        "    df = df.drop(columns=['_description'])\n",
        "\n",
        "    return df, scaler\n",
        "\n",
        "\n",
        "def perform_pca(df_scaled):\n",
        "    \"\"\"\n",
        "    Perform PCA on a preprocessed DataFrame and return PCA transformed DataFrame and PCA model\n",
        "\n",
        "    :parameters:\n",
        "    - Preprocessed DataFrame \n",
        "\n",
        "    :return: \n",
        "    - DataFrame with transformed data\n",
        "    - PCA model\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # Fit PCA model and plot cumulative explained variance\n",
        "    pca = PCA().fit(df_scaled)\n",
        "    plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
        "    plt.xlabel('Number of components')\n",
        "    plt.ylabel('Cumulative explained variance')\n",
        "    plt.show()\n",
        "\n",
        "    # Find the appropriate number of components that explains at least 90% of variance\n",
        "    cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
        "    n_components = np.argmax(cumulative_variance >= 0.90) + 1 \n",
        "    \n",
        "    # Fit PCA model with optimal n_components to transform the data\n",
        "    pca = PCA(n_components=n_components).fit(df_scaled)\n",
        "    pca_transformed_data = pca.transform(df_scaled)\n",
        "    df_transformed = pd.DataFrame(pca_transformed_data, columns=[f'PC{i}' for i in range(1, n_components+1)])\n",
        "\n",
        "    # Print the explained variance for all principal components\n",
        "    print('Explained variance for all principal components:')\n",
        "    print(pca.explained_variance_ratio_)\n",
        "\n",
        "    # Print the cumulative explained variance for all principal components\n",
        "    print('Cumulative explained variance for all principal components:')\n",
        "    print(np.cumsum(pca.explained_variance_ratio_))\n",
        "\n",
        "    # Calculate loadings and feature importance for each principal component\n",
        "    loadings = pd.DataFrame(pca.components_.T, columns=[f'PC{i}' for i in range(1, n_components+1)], index=df_scaled.columns)\n",
        "    abs_sum_loadings = loadings.abs().sum(axis=1).sort_values(ascending=False)\n",
        " \n",
        "    # Define the color to use in chart\n",
        "    color = 'rgba(128, 128, 128, 1)'\n",
        "\n",
        "    # Print the top three important features for each PC\n",
        "    for i in range(1, n_components+1):\n",
        "        print(f'Top three important features for PC{i}:')\n",
        "        print(loadings[f'PC{i}'].abs().nlargest(5))\n",
        "\n",
        "    # Create a vertical bar chart with the most important feature on the left\n",
        "    fig = go.Figure(go.Bar(\n",
        "        x=abs_sum_loadings.values[::-1],\n",
        "        y=abs_sum_loadings.index[::-1],\n",
        "        orientation='h',\n",
        "        marker_color=color\n",
        "    ))\n",
        "\n",
        "    # Set the chart title and axis labels\n",
        "    fig.update_layout(\n",
        "        title='Feature Importance',\n",
        "        xaxis_title='Absolute Sum of Loadings',\n",
        "        yaxis_title='Features',\n",
        "        plot_bgcolor='white',\n",
        "        font=dict(size=11, color='black', family='Arial'),\n",
        "        width=1200, \n",
        "        height=600 \n",
        "    )\n",
        "\n",
        "    fig.show()\n",
        "\n",
        "    return df_transformed, pca\n",
        "\n",
        "\n",
        "def perform_hca_clustering(df_transformed, pca, scaler, original_df):\n",
        "    \"\"\"\n",
        "    Perform HCA on a preprocessed and PCA transformed DataFrame. After clustering, PCA transformation and scaling will be reverted to get true mean values for clusters\n",
        "\n",
        "    :parameters:\n",
        "    - Preprocessed and PCA transformed dataframe\n",
        "    - PCA model\n",
        "    - MinMaxScaler object\n",
        "    - Original DataFrame\n",
        "\n",
        "    :return:\n",
        "    - HCA_means: Cluster means from HCA clustering\n",
        "    - HCA_df: Original DataFrame with cluster labels from HCA attached\n",
        "    - hca_silhouette_score: Cluster means from HCA clustering\n",
        "    \"\"\"\n",
        "    print('\\n')\n",
        "    print(' --------------- HIERARCHICAL CLUSTERING ---------------')\n",
        "\n",
        "    # Set seed for reproducability \n",
        "    random.seed(42)\n",
        "\n",
        "    # Define range of clusters to evaluate\n",
        "    n_clusters_range = range(2, 6)\n",
        "\n",
        "    # Calculate the silhouette score for each number of clusters and store them in scores list\n",
        "    scores = []\n",
        "\n",
        "    # Loop over each number of clusters, perform HCA, and attach silhouette score to scores list\n",
        "    for n_clusters in n_clusters_range:\n",
        "        hca = AgglomerativeClustering(n_clusters=n_clusters)\n",
        "        hca.fit(df_transformed)\n",
        "        labels = hca.labels_\n",
        "        score = silhouette_score(df_transformed, labels)\n",
        "        scores.append(score)\n",
        "\n",
        "    # Plot the silhouette scores across number of clusters to examine closer\n",
        "    plt.plot(n_clusters_range, scores)\n",
        "    plt.xlabel('Number of clusters')\n",
        "    plt.ylabel('Silhouette score')\n",
        "    plt.show()\n",
        "\n",
        "    # Choose the number of clusters with the highest silhouette score\n",
        "    best_n_clusters = n_clusters_range[np.argmax(scores)]\n",
        "    print(f\"Best number of clusters: {best_n_clusters}\")\n",
        "\n",
        "    # Perform hierarchical clustering with best_n_clusters\n",
        "    HCA_clustering = AgglomerativeClustering(n_clusters=best_n_clusters)\n",
        "    HCA_clustering.fit(df_transformed)\n",
        "\n",
        "    # Remove PCA transformation to interpret means of clusters\n",
        "    df_inv = pca.inverse_transform(df_transformed)\n",
        "\n",
        "    # Create a dataframe with the inverse PCA transformation\n",
        "    df_inv_pca = pd.DataFrame(df_inv, columns=original_df.columns)\n",
        "\n",
        "    # Revert MinMax scaling on the relevant columns\n",
        "    columns_of_interest = ['average_monthly_sales','average_order_size','average_value_by_order','unique_customer_per_item','average_monthly_quantity','profit_margin','Net Weight','Reorder Quantity','Unit Cost','First Purch Order Quantum','average_days_between_sales','Customer_life_time','average_monthly_sales_growth','sales_variability']\n",
        "    df_inv_pca[columns_of_interest] = scaler.inverse_transform(df_inv_pca[columns_of_interest])\n",
        "\n",
        "    # Creating a dataframe with the cluster numbers attached and selecting columns of interest\n",
        "    HCA_df = df_inv_pca[columns_of_interest].copy()\n",
        "    HCA_df.loc[:, 'cluster'] = HCA_clustering.labels_\n",
        "    HCA_df.loc[:, 'item_number'] = original_df.index\n",
        "\n",
        "    # Group by cluster and compute mean values for selected columns of interest\n",
        "    HCA_means = HCA_df.groupby('cluster')[columns_of_interest[1:]].mean()\n",
        "\n",
        "    # Calculate final silhouette score for the best HCA clustering\n",
        "    hca_silhouette_score = silhouette_score(df_transformed, HCA_clustering.labels_)\n",
        "\n",
        "    # Print the print cluster means and HCA silhoette score\n",
        "    print('/n')\n",
        "    print('Cluster means:')\n",
        "    print(HCA_means)\n",
        "    print('/n')\n",
        "    print('Silhouette score:')\n",
        "    print(hca_silhouette_score)\n",
        "\n",
        "    return HCA_means, HCA_df, hca_silhouette_score\n",
        "\n",
        "\n",
        "\n",
        "def perform_hdbscan_clustering(df_transformed, pca, scaler, original_df):\n",
        "    \"\"\"\n",
        "    Perform HDBSCAN on a preprocessed and PCA transformed DataFrame. After clustering, PCA transformation and scaling will be reverted to get true mean values for clusters\n",
        "\n",
        "    :parameters:\n",
        "    - Preprocessed and PCA transformed dataframe\n",
        "    - PCA model\n",
        "    - MinMaxScaler object\n",
        "    - Original DataFrame\n",
        "\n",
        "    :return:\n",
        "    - hdbscan_means: Cluster means from HDBSCAN clustering\n",
        "    - hdbscan_df: Original DataFrame with cluster labels from HDBSCAN attached\n",
        "    - hdbscan_silhouette_score: Cluster means from HDBSCAN clustering\n",
        "    \"\"\"\n",
        "    \n",
        "    print('\\n')\n",
        "    print(' --------------- HDBSCAN ---------------')\n",
        "\n",
        "    # Set seed for reproducability \n",
        "    random.seed(42)\n",
        "    \n",
        "    # Tune over the the optimal min_cluster_size and min_samples parameters using the silhouette score as metric\n",
        "    range_min_cluster_size = range(20, 50)\n",
        "    range_min_samples = range(5, 20)\n",
        "    scores = []\n",
        "\n",
        "    # Looping over ranges og hyperparameters, perform HDBSCAN and attach silhouette score to scores object\n",
        "    for min_cluster_size in range_min_cluster_size:\n",
        "        for min_samples in range_min_samples:\n",
        "            hdbscan_clustering = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size, min_samples=min_samples)\n",
        "            hdbscan_clustering.fit(df_transformed)\n",
        "            labels = hdbscan_clustering.labels_\n",
        "            n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
        "            if n_clusters > 1:\n",
        "                score = silhouette_score(df_transformed, labels)\n",
        "                scores.append((min_cluster_size, min_samples, score))\n",
        "            else:\n",
        "                scores.append((min_cluster_size, min_samples, -1))\n",
        "\n",
        "    # Extract best params seperately to use in HDBSCAN function\n",
        "    best_min_cluster_size, best_min_samples, _ = max(scores, key=lambda x: x[2])\n",
        "    print(f'Best min_cluster_size value: {best_min_cluster_size}')\n",
        "    print(f'Best min_samples value: {best_min_samples}')\n",
        "\n",
        "    # Perform HDBSCAN clustering with the optimal parameters\n",
        "    hdbscan_clustering = hdbscan.HDBSCAN(min_cluster_size=best_min_cluster_size, min_samples=best_min_samples)\n",
        "    hdbscan_clustering.fit(df_transformed)\n",
        "\n",
        "    # Remove PCA transformation to interpret means of clusters\n",
        "    df_inv = pca.inverse_transform(df_transformed)\n",
        "\n",
        "    # Create a dataframe with the inverse PCA transformed values\n",
        "    df_inv_pca = pd.DataFrame(df_inv, columns=original_df.columns)\n",
        "\n",
        "    # Revert MinMax scaling on the relevant columns\n",
        "    columns_of_interest = ['average_monthly_sales','average_order_size','average_value_by_order','unique_customer_per_item','average_monthly_quantity','profit_margin','Net Weight','Reorder Quantity','Unit Cost','First Purch Order Quantum','average_days_between_sales','Customer_life_time','average_monthly_sales_growth','sales_variability']\n",
        "    df_inv_pca[columns_of_interest] = scaler.inverse_transform(df_inv_pca[columns_of_interest])\n",
        "\n",
        "    # Creating a dataframe with the cluster numbers attached and selecting columns of interest\n",
        "    hdbscan_df = df_inv_pca[columns_of_interest].copy()\n",
        "    hdbscan_df.loc[:, 'cluster'] = hdbscan_clustering.labels_\n",
        "    hdbscan_df.loc[:, 'item_number'] = original_df.index\n",
        "\n",
        "    # Create a new dataframe with item numbers and their corresponding cluster labels\n",
        "    item_cluster_df = hdbscan_df[['item_number', 'cluster']]\n",
        "\n",
        "    # Group by cluster and compute mean values for selected columns of interest\n",
        "    hdbscan_means = hdbscan_df.groupby('cluster').mean() \n",
        "\n",
        "    # Calculate the silhouette score for the best HDBSCAN clustering\n",
        "    hdbscan_silhouette_score = silhouette_score(df_transformed, hdbscan_clustering.labels_)\n",
        "\n",
        "    # Create a subplot with two scatter plots side by side\n",
        "    fig = make_subplots(rows=1, cols=2, subplot_titles=('PC1 vs. PC2', 'PC3 vs. PC4'))\n",
        "\n",
        "    # Add first scatter plot to the first subplot of PC1 and PC2\n",
        "    fig.add_trace(px.scatter(df_transformed, x='PC1', y='PC2', color=hdbscan_clustering.labels_, hover_name=original_df.index).data[0], row=1, col=1)\n",
        "\n",
        "    # add second scatter plot to the second subplot of PC3 and PC4\n",
        "    fig.add_trace(px.scatter(df_transformed, x='PC3', y='PC4', color=hdbscan_clustering.labels_, hover_name=original_df.index).data[0], row=1, col=2)\n",
        "\n",
        "    # Adjust plot\n",
        "    fig.update_layout(height=500, width=1000, title_text=\"Clustered Data\",\n",
        "                      legend=dict(x=0.5, y=-0.1, orientation='h', title='Cluster Labels'))\n",
        "    \n",
        "    fig.update_layout(plot_bgcolor='rgba(0,0,0,0)')\n",
        "    fig.show()\n",
        "\n",
        "    # Print the print cluster means and HDBSCAN silhoette score\n",
        "    print('/n')\n",
        "    print('Cluster means:')\n",
        "    print(hdbscan_means)\n",
        "    print('/n')\n",
        "    print('Silhouette score:')\n",
        "    print(hdbscan_silhouette_score)\n",
        "\n",
        "    return hdbscan_means, hdbscan_df, hdbscan_silhouette_score\n",
        "\n",
        "\n",
        "\n",
        "def descriptive_clust(hdbscan_df):\n",
        "    \"\"\"\n",
        "    Extract cluster means and create spider diagram statistics from the output DataFrame from the HDBSCAN clustering which includes cluster labels for each SKU\n",
        "\n",
        "    :parameters:\n",
        "    - HDBSCAN DataFrame\n",
        "    \n",
        "    :return:\n",
        "    - Cluster means\n",
        "    - Spider diagram\n",
        "\n",
        "    \"\"\"\n",
        "    \n",
        "    # Group by cluster and compute mean values for selected columns of interest\n",
        "    columns_of_interest = ['average_monthly_sales', 'average_order_size', 'average_value_by_order', 'unique_customer_per_item', 'average_monthly_quantity', 'profit_margin', 'Net Weight', 'Reorder Quantity', 'Unit Cost', 'First Purch Order Quantum', 'average_days_between_sales', 'Customer_life_time', 'average_monthly_sales_growth', 'sales_variability']\n",
        "    hdbscan_means = hdbscan_df.groupby('cluster')[columns_of_interest].mean() \n",
        "\n",
        "    # Scale the data using StandardScaler prior to spider diagram\n",
        "    scaler = MinMaxScaler()\n",
        "    scaled_means = scaler.fit_transform(hdbscan_means)\n",
        "    scaled_means = pd.DataFrame(scaled_means, columns=columns_of_interest, index=hdbscan_means.index)\n",
        "\n",
        "    # Filter out cluster -1 (outliers)\n",
        "    scaled_means = scaled_means.drop(index=-1)\n",
        "\n",
        "    # Define a dictionary to map each cluster to a specific color\n",
        "    cluster_colors = {-1: 'rgba(41, 57, 71, 1)',\n",
        "                      0: 'rgba(93, 112, 127, 1)',\n",
        "                      1: 'rgba(80, 177, 200, 100)',\n",
        "                      2: 'rgba(154, 177, 197, 100)',\n",
        "                      3: 'rgba(70, 115, 171, 100)',\n",
        "                      4: 'rgba(28, 59, 95, 100)'}\n",
        "\n",
        "\n",
        "    # Create spider diagram\n",
        "    fig = go.Figure()\n",
        "    for i, cluster in enumerate(scaled_means.index):\n",
        "        values = scaled_means.loc[cluster].values.tolist()\n",
        "        fig.add_trace(go.Scatterpolar(\n",
        "            r=values,\n",
        "            theta=columns_of_interest,\n",
        "            fill='toself',\n",
        "            name=f'Cluster {cluster}',\n",
        "            line=dict(width=3),\n",
        "            marker=dict(size=8, color=cluster_colors[cluster]), \n",
        "            opacity=0.7, \n",
        "            fillcolor=cluster_colors[cluster] \n",
        "        ))\n",
        "\n",
        "    fig.update_layout(\n",
        "        polar=dict(\n",
        "            radialaxis=dict(\n",
        "                visible=True,\n",
        "                range=[0, 1]  \n",
        "            )\n",
        "        ),\n",
        "\n",
        "        title=dict(\n",
        "            text='Cluster Means - Spider Diagram',\n",
        "            x=0.5, \n",
        "            y=0.95 \n",
        "        ),\n",
        "\n",
        "        plot_bgcolor='rgba(255, 255, 255, 1)',\n",
        "        height=800 \n",
        "    )\n",
        "\n",
        "    fig.update_layout(\n",
        "        legend=dict(orientation='h', yanchor='bottom', y=-0.1, xanchor='center', x=0.5),\n",
        "        legend_title=\"\"\n",
        "    )\n",
        "\n",
        "    fig.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFxHZRU--Llc"
      },
      "source": [
        "# Clustering without PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2os3FreN31NE"
      },
      "outputs": [],
      "source": [
        "def main_wo_pca():\n",
        "    \"\"\"\n",
        "    Main function for clustering analysis excluding PCA, including all steps necessary.\n",
        "    Change path to data and allow access to Gdrive for the function to execute.\n",
        "\n",
        "    :parameters: \n",
        "    - None\n",
        "    \n",
        "    :return: \n",
        "    - hca_means: Cluster means from HCA clustering\n",
        "    - hca_df: Original DataFrame with cluster labels from HCA attached\n",
        "    - hca_ss: HCA clustering silhouette score\n",
        "    - hdbscan_means: Cluster means from HDBSCAN clustering\n",
        "    - hdbscan_df: Original DataFrame with cluster labels from HDBSCAN attached\n",
        "    - hdbscan_ss: Cluster means from HDBSCAN clustering\n",
        "    \"\"\"\n",
        "\n",
        "    random.seed(42)\n",
        "    \n",
        "    # Load and preprocess data\n",
        "    df = pd.read_csv('/content/drive/MyDrive/Master Thesis/final_updated_clustering_data.csv')\n",
        "\n",
        "    # Preprocess data\n",
        "    df_preprocessed, scaler = preprocess(df)\n",
        "\n",
        "    # Perform HCA clustering\n",
        "    hca_means, hca_df, hca_ss = perform_hca_clustering(df_preprocessed, scaler, df_preprocessed)\n",
        "\n",
        "    # Perform DBSCAN clustering\n",
        "    hdbscan_means, hdbscan_df,hdbscan_ss = perform_hdbscan_clustering(df_preprocessed, scaler, df_preprocessed)\n",
        "\n",
        "    return hca_means, hca_df, hca_ss, hdbscan_means, hdbscan_df,hdbscan_ssX\n",
        "\n",
        "\n",
        "def text_preprocessing(df):\n",
        "    \"\"\"\n",
        "    Preprocessing function for textual data to extract additional information through clustering\n",
        "\n",
        "    :parameters:\n",
        "    - Dataframe with column '_description' containing textual data\n",
        "\n",
        "    :return: \n",
        "    - DataFrame with an additional column containing text cluster labels and wordclouds for visualization of the data\n",
        "    \"\"\"\n",
        "\n",
        "    # Initiate constant objects\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    stemmer = PorterStemmer()\n",
        "\n",
        "    # Tokenize the text, remove punctuation and stopwords, and stem words\n",
        "    df['_description'] = df['_description'].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)).lower().split())\n",
        "    df['_description'] = df['_description'].apply(lambda x: [word for word in x if not word in stop_words])\n",
        "    df['_description'] = df['_description'].apply(lambda x: [stemmer.stem(word) for word in x])\n",
        "\n",
        "    # Perform TF-IDF weighting on the text column\n",
        "    tfidf_vectorizer = TfidfVectorizer()\n",
        "    tfidf_data = tfidf_vectorizer.fit_transform(df['_description'].apply(lambda x: \" \".join(x)))\n",
        "\n",
        "    # Dimensionality reduction using UMAP\n",
        "    umap_model = umap.UMAP(n_neighbors=15, min_dist=0.5, metric='euclidean', random_state=42)\n",
        "    umap_data = umap_model.fit_transform(tfidf_data)\n",
        "\n",
        "    # Clustering using HDBSCAN\n",
        "    clusterer = hdbscan.HDBSCAN(min_cluster_size=10)\n",
        "    cluster_labels = clusterer.fit_predict(umap_data)\n",
        "\n",
        "    # Add clustered column to the DataFrame\n",
        "    df['cluster'] = cluster_labels\n",
        "\n",
        "    # Extract 10 most common words for each cluster + create wordcloud for each cluster with 50 most common words\n",
        "    for label in df['cluster'].unique():\n",
        "\n",
        "        print(f\"Cluster {label}:\")\n",
        "        cluster_df = df.loc[df['cluster'] == label]\n",
        "        words = [word for doc in cluster_df['_description'] for word in doc]\n",
        "\n",
        "        # Find 10 most common words\n",
        "        word_counts = collections.Counter(words)\n",
        "        top_10_words = word_counts.most_common(10)\n",
        "        print('Top 10 words in clusters:')\n",
        "        print(top_10_words)\n",
        "\n",
        "        # Create word cloud\n",
        "        print(f'Wordcloud for cluster {label}')\n",
        "        word_counts = collections.Counter(words)\n",
        "        top_50_words = word_counts.most_common(50)\n",
        "        top_words_dict = dict(top_50_words)\n",
        "        wordcloud = WordCloud(width = 800, height = 800, background_color ='white', stopwords = stop_words, min_font_size = 10).generate_from_frequencies(top_words_dict)\n",
        "\n",
        "        # Plot word cloud\n",
        "        plt.figure(figsize = (8, 8), facecolor = None)\n",
        "        plt.imshow(wordcloud)\n",
        "        plt.axis(\"off\")\n",
        "        plt.tight_layout(pad = 0)\n",
        "        plt.show()\n",
        "\n",
        "    return df\n",
        "\n",
        "def preprocess(df):\n",
        "  \"\"\"\n",
        "  Dedicated preprocessing function for handling numerical, categorical, and textual data (:using dedicated text_preprocessing function)\n",
        "\n",
        "  :parameters:\n",
        "  - DataFrame used for clustering\n",
        "\n",
        "  :return:\n",
        "  - Preprocessed DataFrame\n",
        "  - Scaler object from MinMaxScaler for later inverting\n",
        "  \"\"\"\n",
        "\n",
        "  # Removing all -inf profit_margins \n",
        "  df = df[(df['profit_margin'] >= 0) & (df['profit_margin'] != float('inf')) & (df['profit_margin'] != float('-inf'))]\n",
        "\n",
        "  # Drop redundant columns\n",
        "  exclude_columns = [\"Unnamed: 0\",\"BDLRI Sales Status\"]\n",
        "  df = df.loc[:, ~df.columns.isin(exclude_columns)]\n",
        "\n",
        "  # One-hot encode categorical variables\n",
        "  df = pd.concat([df, pd.get_dummies(df['Country_Region of Origin Code'], prefix='Country')], axis=1).drop('Country_Region of Origin Code', axis=1)\n",
        "\n",
        "  # Preprocess description using seperate function\n",
        "  df = text_preprocessing(df)\n",
        "  \n",
        "  # Scale the columns separately\n",
        "  scaler = MinMaxScaler()\n",
        "  df[['average_monthly_sales','average_order_size','average_value_by_order','unique_customer_per_item','average_monthly_quantity','profit_margin','Net Weight','Reorder Quantity','Unit Cost','First Purch Order Quantum','average_days_between_sales','Customer_life_time','average_monthly_sales_growth','sales_variability']] = scaler.fit_transform(df[['average_monthly_sales','average_order_size','average_value_by_order','unique_customer_per_item','average_monthly_quantity','profit_margin','Net Weight','Reorder Quantity','Unit Cost','First Purch Order Quantum','average_days_between_sales','Customer_life_time','average_monthly_sales_growth','sales_variability']])\n",
        "\n",
        "  # remove and handlenan rows\n",
        "  df = df.dropna(subset=['_ItemNumber'])\n",
        "  df = df.fillna(0)\n",
        "  df = df.set_index('_ItemNumber')\n",
        "  df.columns = df.columns.astype(str)\n",
        "\n",
        "  # Create a new DataFrame with the original _description column and cluster labels\n",
        "  description_clusters = df.reset_index()[['_ItemNumber', '_description', 'cluster']]\n",
        "\n",
        "  # Group the DataFrame by cluster labels\n",
        "  grouped_descriptions = description_clusters.groupby('cluster')\n",
        "\n",
        "  # Drop description at is has been preprocessed earlier\n",
        "  df = df.drop(columns=['_description'])\n",
        "\n",
        "  return df, scaler\n",
        "\n",
        "\n",
        "\n",
        "def perform_hca_clustering(df_transformed, scaler, original_df):\n",
        "    \"\"\"\n",
        "    Perform HCA on a preprocessed DataFrame. After clustering scaling will be reverted to get true mean values for clusters\n",
        "\n",
        "    :parameters:\n",
        "    - Preprocessed dataframe\n",
        "    - MinMaxScaler object\n",
        "    - Original DataFrame\n",
        "\n",
        "    :return:\n",
        "    - HCA_means: Cluster means from HCA clustering\n",
        "    - HCA_df: Original DataFrame with cluster labels from HCA attached\n",
        "    - HCA_silhouette_score: Cluster means from HCA clustering\n",
        "    \"\"\"\n",
        "    \n",
        "    print('\\n')\n",
        "    print(' --------------- HIERARCHICAL CLUSTERING ---------------')\n",
        "\n",
        "    # Set seed for reproducability \n",
        "    random.seed(42)\n",
        "\n",
        "    # Define range of number of clusters to evaluate\n",
        "    n_clusters_range = range(2, 6)\n",
        "\n",
        "    # Calculate the silhouette score for each cluster number\n",
        "    scores = []\n",
        "\n",
        "    # Loop over each number of clusters, perform HCA, and attach silhouette score to scores list\n",
        "    for n_clusters in n_clusters_range:\n",
        "        hca = AgglomerativeClustering(n_clusters=n_clusters)\n",
        "        hca.fit(df_transformed)\n",
        "        labels = hca.labels_\n",
        "        score = silhouette_score(df_transformed, labels)\n",
        "        scores.append(score)\n",
        "\n",
        "    # Plot the silhouette scores across number of clusters to examine closer\n",
        "    plt.plot(n_clusters_range, scores)\n",
        "    plt.xlabel('Number of clusters')\n",
        "    plt.ylabel('Silhouette score')\n",
        "    plt.show()\n",
        "\n",
        "    # Choose the number of clusters with the highest cross-validation score\n",
        "    best_n_clusters = n_clusters_range[np.argmax(scores)]\n",
        "    print(f\"Best number of clusters: {best_n_clusters}\")\n",
        "\n",
        "    # Perform hierarchical clustering with best_n_clusters\n",
        "    HCA_clustering = AgglomerativeClustering(n_clusters=best_n_clusters)\n",
        "    HCA_clustering.fit(df_transformed)\n",
        "\n",
        "    # Revert MinMax scaling only on the relevant columns\n",
        "    columns_of_interest = ['average_monthly_sales','average_order_size','average_value_by_order','unique_customer_per_item','average_monthly_quantity','profit_margin','Net Weight','Reorder Quantity','Unit Cost','First Purch Order Quantum','average_days_between_sales','Customer_life_time','average_monthly_sales_growth','sales_variability']\n",
        "    df_transformed[columns_of_interest] = scaler.inverse_transform(df_transformed[columns_of_interest])\n",
        "\n",
        "    # Creating a dataframe with the cluster numbers attached and selecting columns of interest\n",
        "    HCA_df = df_transformed[columns_of_interest].copy()\n",
        "    HCA_df.loc[:, 'cluster'] = HCA_clustering.labels_\n",
        "    HCA_df.loc[:, 'item_number'] = original_df.index\n",
        "\n",
        "    # Group by cluster and compute mean values for selected columns of interest\n",
        "    HCA_means = HCA_df.groupby('cluster')[columns_of_interest[1:]].mean()\n",
        "\n",
        "    # Calculate final silhouette score for the best HCA clustering\n",
        "    hca_silhouette_score = silhouette_score(df_transformed, HCA_clustering.labels_)\n",
        "\n",
        "    # Print the print cluster means and HCA silhoette score\n",
        "    print('/n')\n",
        "    print('Cluster means:')\n",
        "    print(HCA_means)\n",
        "    print('/n')\n",
        "    print('Silhouette score:')\n",
        "    print(hca_silhouette_score)\n",
        "\n",
        "    return HCA_means, HCA_df, hca_silhouette_score\n",
        "\n",
        "\n",
        "\n",
        "def perform_hdbscan_clustering(df_transformed, scaler, original_df):\n",
        "    \"\"\"\n",
        "    Perform HDBSCAN on a preprocessed DataFrame. After clustering scaling will be reverted to get true mean values for clusters\n",
        "\n",
        "    :parameters:\n",
        "    - Preprocessed dataframe\n",
        "    - MinMaxScaler object\n",
        "    - Original DataFrame\n",
        "\n",
        "    :return:\n",
        "    - HDBSCAN_means: Cluster means from HDBSCAN clustering\n",
        "    - HDBSCAN_df: Original DataFrame with cluster labels from HDBSCAN attached\n",
        "    - HDBSCAN_silhouette_score: Cluster means from HDBSCAN clustering\n",
        "    \"\"\"\n",
        "    print('\\n')\n",
        "    print(' --------------- HDBSCAN ---------------')\n",
        "\n",
        "    # Set seed for reproducability \n",
        "    random.seed(42)\n",
        "    \n",
        "    # Tune over the the optimal min_cluster_size and min_samples parameters using the silhouette score as metric\n",
        "    range_min_cluster_size = range(20, 50)\n",
        "    range_min_samples = range(5, 20)\n",
        "    scores = []\n",
        "\n",
        "    # Looping over ranges og hyperparameters, perform HDBSCAN and attach silhouette score to scores object\n",
        "    for min_cluster_size in range_min_cluster_size:\n",
        "        for min_samples in range_min_samples:\n",
        "            hdbscan_clustering = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size, min_samples=min_samples)\n",
        "            hdbscan_clustering.fit(df_transformed)\n",
        "            labels = hdbscan_clustering.labels_\n",
        "            n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
        "            if n_clusters > 1:\n",
        "                score = silhouette_score(df_transformed, labels)\n",
        "                sil_scores.append((min_cluster_size, min_samples, score))\n",
        "            else:\n",
        "                sil_scores.append((min_cluster_size, min_samples, -1))\n",
        "\n",
        "    # Extract best params seperately to use in HDBSCAN function\n",
        "    best_min_cluster_size, best_min_samples, _ = max(sil_scores, key=lambda x: x[2])\n",
        "    print(f'Best min_cluster_size value: {best_min_cluster_size}')\n",
        "    print(f'Best min_samples value: {best_min_samples}')\n",
        "\n",
        "    # Perform HDBSCAN clustering with the optimal parameters\n",
        "    hdbscan_clustering = hdbscan.HDBSCAN(min_cluster_size=best_min_cluster_size, min_samples=best_min_samples)\n",
        "    hdbscan_clustering.fit(df_transformed)\n",
        "\n",
        "    # Revert MinMax scaling only on the relevant columns\n",
        "    columns_of_interest = ['average_monthly_sales','average_order_size','average_value_by_order','unique_customer_per_item','average_monthly_quantity','profit_margin','Net Weight','Reorder Quantity','Unit Cost','First Purch Order Quantum','average_days_between_sales','Customer_life_time','average_monthly_sales_growth','sales_variability']\n",
        "    df_transformed[columns_of_interest] = scaler.inverse_transform(df_transformed[columns_of_interest])\n",
        "\n",
        "    # Creating a dataframe with the cluster numbers attached and selecting columns of interest\n",
        "    hdbscan_df = df_transformed[columns_of_interest].copy()\n",
        "    hdbscan_df.loc[:, 'cluster'] = hdbscan_clustering.labels_\n",
        "    hdbscan_df.loc[:, 'item_number'] = original_df.index\n",
        "\n",
        "    # Create a new dataframe with item numbers and their corresponding cluster labels\n",
        "    item_cluster_df = hdbscan_df[['item_number', 'cluster']]\n",
        "\n",
        "    # Group by cluster and compute mean values for selected columns of interest\n",
        "    hdbscan_means = hdbscan_df.groupby('cluster')[columns_of_interest].mean()  # -1 are outliers\n",
        "\n",
        "    # Calculate the silhouette score for the best HDBSCAN clustering\n",
        "    hdbscan_silhouette_score = silhouette_score(df_transformed, hdbscan_clustering.labels_)\n",
        "\n",
        "    # Print the print cluster means and HDBSCAN silhoette score\n",
        "    print('/n')\n",
        "    print('Cluster means:')\n",
        "    print(hdbscan_means)\n",
        "    print('/n')\n",
        "    print('Silhouette score:')\n",
        "    print(hdbscan_silhouette_score)\n",
        "\n",
        "    return hdbscan_means, hdbscan_df, hdbscan_silhouette_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zV0lQ3X9QcFu"
      },
      "source": [
        "# Composite index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "37AsvUUNQtSZ"
      },
      "outputs": [],
      "source": [
        "def calculate_composite_index(df):\n",
        "    \"\"\"\n",
        "    Function for calculating and plotting composite index mapping of two dimensions.\n",
        "    Weightings should be updated accordingly\n",
        "\n",
        "    :parameters:\n",
        "    - DataFrame used for composite index\n",
        "\n",
        "    :return:\n",
        "    - 2D plot of clusters\n",
        "    - Full cluster DataFrame including two new columns, one with impact score and one with risk score\n",
        "    - DataFrame with impact/risk scores for all clusters\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    # Initiate weights for the respective dimensions\n",
        "    impact_weights = {'Customer_life_time': 0.25, 'average_monthly_sales': 0.20, 'average_monthly_quantity': 0.15,\n",
        "                      'average_value_by_order': 0.15, 'profit_margin': 0.10, 'average_monthly_sales_growth': 0.05,\n",
        "                      'average_order_size': 0.05, 'Unit Cost': 0.05}\n",
        "\n",
        "    risk_weights = {'average_days_between_sales': 0.25, 'sales_variability': 0.25, 'unique_customer_per_item': 0.25,\n",
        "                    'Net Weight': 0.05, 'Reorder Quantity': 0.05, 'First Purch Order Quantum': 0.05}\n",
        "\n",
        "    # Scale the features\n",
        "    scaler = MinMaxScaler()\n",
        "    scaled_df = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
        "\n",
        "    # Calculate scores\n",
        "    impact_score = (scaled_df[list(impact_weights.keys())] * list(impact_weights.values())).sum(axis=1) / sum(list(impact_weights.values()))\n",
        "    risk_score = (scaled_df[list(risk_weights.keys())] * list(risk_weights.values())).sum(axis=1) / sum(list(risk_weights.values()))\n",
        "\n",
        "    # Adjust the index of impact_score and risk_score\n",
        "    impact_score.index -= 1\n",
        "    risk_score.index -= 1\n",
        "\n",
        "    # Create a new dataframe with the cluster means and their impact and risk scores\n",
        "    result_df = pd.concat([df, impact_score, risk_score], axis=1)\n",
        "    result_df.columns = list(df.columns) + ['Impact score', 'Risk score']\n",
        "\n",
        "    # Create a new dataframe with just the index and the impact and risk scores\n",
        "    result_impact_risk = result_df[['Impact score', 'Risk score']]\n",
        "    \n",
        "    # Define a dictionary to map each cluster to a specific color\n",
        "    cluster_colors = {-1: 'rgba(41, 57, 71, 1)',\n",
        "                      0: 'rgba(128, 128, 128, 1)',\n",
        "                      1: 'rgba(93, 112, 127, 1)',\n",
        "                      2: 'rgba(155, 170, 181, 1)',\n",
        "                      3: 'rgba(80, 177, 200, 100)',\n",
        "                      4: 'rgba(70, 115, 171, 100)'}\n",
        "\n",
        "    # Reset the index of the result_df to make the index a column\n",
        "    result_df = result_df.reset_index()\n",
        "\n",
        "    # Create a scatter plot with color-coded points for each cluster\n",
        "    fig = px.scatter(result_df, x='Impact score', y='Risk score', color='index',\n",
        "                    color_discrete_map=cluster_colors, width=600, height=600)\n",
        "\n",
        "    # Update the layout of the scatter plot\n",
        "    fig.update_layout(\n",
        "        plot_bgcolor='white',\n",
        "        legend=dict(\n",
        "            orientation='h',\n",
        "            yanchor='bottom',\n",
        "            y=1.05,\n",
        "            xanchor='right',\n",
        "            x=1\n",
        "        ),\n",
        "        xaxis=dict(range=[0, 1]),\n",
        "        yaxis=dict(range=[0, 1])\n",
        "    )\n",
        "    fig.update_traces(marker=dict(size=8))\n",
        "    fig.update_traces(marker={'size': 25})\n",
        "    fig.show()\n",
        "\n",
        "    \n",
        "    return result_df, result_impact_risk\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QLobCPJ90qPG"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "-GCeMn2umIjy"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}